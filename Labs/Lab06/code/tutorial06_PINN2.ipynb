{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Networks (PINN): Tutorial - V2\n",
    "> For the Fall 2024 course: *Machine Learning in Physics*<br>\n",
    "> Created: Aug. 14, 2024 Harrison B. Prosper\n",
    "\n",
    "## Introduction\n",
    "Most differential equations cannot be solved exactly. Therefore, over the centuries many numerical methods have been devised to solve these equations approximately. A relatively recent numerical approach called **physics-informed neural networks** (PINN) formulates the solution of a differential equation as an optimization problem in which the approximate solution is modeled using one or more neural networks. For an excellent introduction, I recommend the lectures by Bob Moseley (see, for example, https://www.youtube.com/watch?v=IDIv92Z6Qvc). For a thorough review, see for example, https://doi.org/10.48550/arXiv.2201.05624. \n",
    "Physics-informed neural network is clearly a misnomer. Why? Because this very powerful idea for numerically solving a differential equation can be applied to *any* differential equation, not only the ones in physics! The approach is feasible because the derivatives that enter the objective function can be computed *exactly* using **automatic differentiation**.\n",
    "\n",
    "In this tutorial we consider the example presented by Bob Moseley in the lecture cited above, namely the solution $u(t) \\in \\mathbb{R}$, $t \\in \\mathbb{R}^+$, of the ordinary differential equation (ODE)\n",
    "\\begin{align}\n",
    "   \\frac{d^2 u}{dt^2} + 2 \\lambda \\frac{d u}{ dt } + \\omega_0^2 u = 0,\n",
    "\\end{align}\n",
    "describing a 1D **damped harmomic oscillator**, where \n",
    "\\begin{align}\n",
    "    \\lambda & = \\frac{\\mu}{m} \\quad\\text{and}\\\\\n",
    "    \\omega_0 & = \\sqrt{\\frac{k}{m}}\n",
    "\\end{align}\n",
    "are the characteristic decay rate of the oscillation amplitude and the characteristic frequency, respectively. (In Bob Moseley's lecture $\\lambda = \\delta$.) The quantity $m$ is the mass, $k$ the spring constant and $\\mu$ the coefficient of kinetic friction. \n",
    "\n",
    "### Exact Solution\n",
    "It is almost always a good idea to try to simplify a differential equation before solving it. We can simplify the ODE by dividing throughout by $\\omega_0^2$ and defining the dimensionless quantities $z = \\omega_0 \\, t$ and $\\alpha = \\lambda /\\omega_0 $. With these changes the ODE becomes\n",
    "\\begin{align}\n",
    "  \\frac{d^2 u}{d z^2}  +  2 \\alpha \\frac{d u}{dz} + u = 0 ,\n",
    "\\end{align}\n",
    "which shows that the damped harmonic oscillator is governed by a single parameter $\\alpha$. To obtain a particular solution of the equation we must specify initial conditions\n",
    "\\begin{align}\n",
    "    u(0) & = u_0, \\\\\n",
    "    \\left. \\frac{d u}{dz} \\right|_{z=0} & = d_0 . \n",
    "\\end{align}\n",
    "Since this is a linear equation, we can try a solution of the form\n",
    "\\begin{align}\n",
    "    u(z) & = A \\exp(i q z + \\phi),\n",
    "\\end{align}\n",
    "where $A$ and $\\phi$ are constants to be determined by the initial conditions and where it is understood that we'll take the real part of $u(z)$ at the end. One finds $q = i \\alpha \\pm \\sqrt{1 - \\alpha^2}$. Taking the positive value of $\\sqrt{1 - \\alpha^2}$ and restricting to the condition $\\alpha < 1$ the exact solution is given by\n",
    "\\begin{align}\n",
    "    u(z) & = A \\exp(-\\alpha z) \\, \\cos\\left(z \\sqrt{1-\\alpha^2} + \\phi\\right)  \\quad\\text{and} \\\\\n",
    "    \\phi & = \\arctan\\left(-\\frac{(\\alpha + d_0 / u_0)}{\\sqrt{1 - \\alpha^2}}\\right), \\quad A = \\frac{u_0}{\\cos\\phi} .\n",
    "\\end{align}\n",
    "In terms of the original parameters, \n",
    " the exact solution of the ODE for $\\alpha < 1$, that is, $\\omega_0 > \\lambda$, is \n",
    "\\begin{align}\n",
    "    u(t) & = A \\exp(-\\lambda t) \\, \\cos(\\omega \\, t + \\phi), \\quad\\text{with } \\omega = \\sqrt{\\omega_0^2 - \\lambda^2} \\quad\\text{and} \\\\\n",
    "    \\phi & = \\arctan(-(\\lambda + d_0 / u_0) \\, / \\, \\omega), \\quad A = u_0 \\, / \\, \\cos\\phi , \n",
    "\\end{align}\n",
    "with $d_0$ redefined to be $d_0 = du(0) / dt$.\n",
    "\n",
    "## Physics-Informed Neural Networks\n",
    "Automatic differentiation is the key technology that has enabled the use of increasingly complex models in contemporary machine learning. The same technology can be used to compute derivatives with respect to the inputs of a model. However, when the computation graph is extended to incorporate such derivatives this can lead to an explosion in the size of the graph, which makes solving differential equations through this method considerably slower than algorithms that use finite differences. But if we condition the solution on the initial conditions $u_0$ and $d_0$ the high cost of finding the solutions using stochastic gradient descent for some subset of initial conditions  is *amortized* over the subsequent rapid application of the PINN for any initial condition within that subset. IN effect, the PINN approach can be used to generate an *infinite* number of solutions indexed by the parameters that define the initial conditions. In a finite-difference calculation, a different calculation is needed for each set of initial (or boundary) conditions. A PINN conditioned on the initial conditions is sometimes called a **conditioned PINN** (C-PINN).  One of the nice features of PINNs is that their solutions can be checked at a few selected parameter points by comparing PINN solutions with solutions obtained using finite-difference methods.\n",
    "\n",
    "The PINN approach applied to the damped harmonic oscillator entails solving the differential equation by minimizing the following empirical risk, or objective, function\n",
    "\\begin{align}\n",
    "    O(\\theta) & = \\frac{1}{N} \\sum_{i=1}^N \\left(  \\frac{d^2u_i}{dz_i^2} + 2 \\alpha \\frac{du_i}{dz_i} + u_i \\right)^2, \\quad\\quad\n",
    "    u_i  \\equiv u(z_i) .\n",
    "\\end{align}\n",
    "The $z_i$ are called **collocation points**. In the standard PINN approach, additional terms are added to incorporate the initial and/or boundary conditions and any additional data at hand. But in this tutorial, we have no additional data and we choose to incorporate the initial conditions explicitly using the *ansatz*\n",
    "\\begin{align}\n",
    "    u(z)  & = \\frac{u_0 + z \\, d_0 + z^2 \\, g(z, u_0, d_0, \\alpha; \\theta)}{1+z^2} ,\n",
    "\\end{align}\n",
    "where $g(z, u_0, d_0, \\alpha; \\theta)$ is a deep neural network with free parameters $\\theta$. This ansatz has the property that $u(z) \\rightarrow g(z,\\cdots)$ and $du/dz \\rightarrow dg(z,\\cdots)/dz$ as $z \\rightarrow \\infty$. Our goal is to find a *continuous* set of solutions in the domain $(z, u_0, d_0, \\alpha) \\in [0, 20] \\otimes [-1, 1] \\otimes [-2, 2] \\otimes [0, 0.5] \\subset \\mathbb{R}^4$.\n",
    "\n",
    "### Difference with respect to other version of the tutorial\n",
    "In the other version of the tutorial, we defined $x = (z, u_0, d_0, \\alpha)$. Doing so forced us to take the derivative of $u$ with respect to $z$, $u_0$, $d_0$, and $\\alpha$, which is not what we want; we want only the derivative with respect to $z$. \n",
    "\n",
    "In this version of the tutorial, we split $x$ into $z \\in \\mathbb{R}$ and $x \\in \\mathbb{R}^3$ and feed $z$ and $x$ into two different deep neural networks whose outputs are combined into a third neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard system modules\n",
    "import os, sys\n",
    "sys.path.append('../../python')\n",
    "\n",
    "# standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# standard module for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard research-level machine learning toolkit from Meta (FKA: FaceBook)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# needed to generate Sobol sequences of points (quasi-random sampling)\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# a simple time left function.\n",
    "# to monitor the training do\n",
    "# python monitor_losses.py losses.csv\n",
    "from lossmonitor import TimeLeft\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 12\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set usetex = False if LaTex is not \n",
    "# available on your system or if the \n",
    "# rendering is too slow\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tComputational device: cpu \n",
      "\n",
      "Available device: cpu \n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\n\\tComputational device: {str(device):4s}\\n')\n",
    "print(f'Available device: {str(device):4s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solution\n",
    "\\begin{align}\n",
    "    u(z) & = A \\exp(-\\alpha z) \\, \\cos\\left(z \\sqrt{1-\\alpha^2} + \\phi\\right)  \\quad\\text{and} \\\\\n",
    "    \\phi & = \\arctan\\left(-\\frac{(\\alpha + d_0 / u_0)}{\\sqrt{1 - \\alpha^2}}\\right), \\quad A = \\frac{u_0}{\\cos\\phi} .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(z, u0, d0, alpha):\n",
    "    assert alpha > 0\n",
    "    assert alpha < 1\n",
    "    a = np.sqrt(1 - alpha**2)\n",
    "    phi = np.arctan(-(alpha + d0/u0)/a)\n",
    "    A = u0 / np.cos(phi)\n",
    "    u = A * torch.exp(-alpha*z) * torch.cos(a*z + phi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation, and test sets\n",
    "There is some confusion in terminology regarding validation and test samples (or sets). We shall adhere to the defintions given here https://machinelearningmastery.com/difference-test-validation-datasets/):\n",
    "   \n",
    "  * __Training Dataset__: The sample of data used to fit the model.\n",
    "  * __Validation Dataset__: The sample of data used to decide 1) whether the fit is reasonable (e.g., the model has not been overfitted), 2) decide which of several models is the best and 3) tune model hyperparameters.\n",
    "  * __Test Dataset__: The sample of data used to provide an unbiased evaluation of the fitted model.\n",
    "\n",
    "The validation set will be some small fraction of the training set and will be used to decide when to stop the training.\n",
    "\n",
    "## Generate data\n",
    "Here we generate a quasi-random set of points $x = (z, u_0, d_0, \\alpha) \\in S \\subset \\mathbb{R}^4$. Quasi-random sampling disributes points more evenly than uniform random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([131072, 1]), torch.Size([131072, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sobol_sequence(D, P, lower, upper):\n",
    "    # D: dimension of input space\n",
    "    # P: number of points = 2^P\n",
    "    \n",
    "    # generate 2^P points in D-dimensional unit hypercube\n",
    "    sampler= qmc.Sobol(d=D, scramble=True)\n",
    "    sample = sampler.random_base2(m=P)\n",
    "\n",
    "    # scale to desired size of hyperrectangle\n",
    "    sample = qmc.scale(sample, lower, upper)\n",
    "\n",
    "    return sample\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# CONSTANTS\n",
    "# -----------------------------------------------------------\n",
    "LOSS_FILE     = 'losses.csv'   # average losses saved here\n",
    "PARAMS_FILE   = 'model.pth'    # model parameters\n",
    "PLOT_FILE     = 'results.png'\n",
    "TIMELEFT_FILE = 'timeleft.txt' # time left to training completion\n",
    "\n",
    "# define domain of C-PINN\n",
    "#           z,  u0,  d0, alpha\n",
    "LOWER = [ 0.0,-1.0,-2.0, 0.0]\n",
    "UPPER = [20.0, 1.0, 2.0, 0.5]\n",
    "SCALE = [20.0, 1.0, 2.0, 1.0]    # See Solution class below\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# DATA\n",
    "# -----------------------------------------------------------\n",
    "X = sobol_sequence(len(LOWER), 17, LOWER, UPPER)\n",
    "\n",
    "# split into z and x data\n",
    "Z, X = X[:, 0], X[:, 1:]\n",
    "\n",
    "# alert PyTorch that we intend to differentiate a function with respect to z\n",
    "Z = torch.Tensor(Z).requires_grad_(True).view(-1, 1).to(device) # need shape (batch_size, 1)\n",
    "X = torch.Tensor(X).to(device)  \n",
    "\n",
    "Z.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120000, 1]),\n",
       " torch.Size([120000, 3]),\n",
       " torch.Size([5000, 3]),\n",
       " torch.Size([6072, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrain = min(120000, len(X))\n",
    "nvalid = 5000\n",
    "\n",
    "i = 0; j= i + ntrain\n",
    "train_Z = Z[i:j]\n",
    "train_X = X[i:j]\n",
    "\n",
    "i += ntrain; j = i + nvalid\n",
    "valid_Z = Z[i:j]\n",
    "valid_X = X[i:j]\n",
    "\n",
    "i += nvalid\n",
    "test_Z = Z[i:j]\n",
    "test_X = X[i:]\n",
    "\n",
    "train_Z.shape, train_X.shape, valid_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Deep Neural Network Model\n",
    "\n",
    "<img src=\"./pinn2.png\" width=\"400px\"/> \n",
    "\n",
    "### $\\texttt{SiLU}(x)$\n",
    "\n",
    "$\\text{silu}(x) = x \\, \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_width,\n",
    "                 nonlinearity=nn.SiLU):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # network for z\n",
    "        cmd  = 'nn.Sequential(nn.Linear(1, n_width), nonlinearity(), '\n",
    "        cmd += ', '.join(['nn.Linear(n_width, n_width), nonlinearity()' \n",
    "                          for _ in range(n_hidden)])\n",
    "        cmd += ')'\n",
    "        self.znet = eval(cmd)\n",
    "\n",
    "        # network for q = u0, d0, alpha\n",
    "        cmd  = 'nn.Sequential(nn.Linear(n_input-1, n_width), nonlinearity(), '\n",
    "        cmd += ', '.join(['nn.Linear(n_width, n_width), nonlinearity()' \n",
    "                          for _ in range(n_hidden)])\n",
    "        cmd += ')'\n",
    "        self.xnet = eval(cmd)\n",
    "\n",
    "        self.cnet = nn.Sequential(nn.Linear(2*n_width, n_width), nonlinearity(),\n",
    "                                  nn.Linear(n_width, 1))\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        z = self.znet(z)\n",
    "        x = self.xnet(x)\n",
    "        x = torch.cat([z, x], dim=1) # concatenate horizontally\n",
    "        x = self.cnet(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution ansatz\n",
    "We'll use the following solution ansatz\n",
    "\\begin{align}\n",
    "    u(z)  & = \\frac{u_0 + z \\, d_0 + z^2 \\, g(x; \\theta)}{1+z^2}, \\\\\n",
    "    \\frac{du(z)}{dz} & = \\frac{-2 z \\, u_0 + (1-z^2) \\, d_0 + 2z \\, g(x;\\theta)}{(1+z^2)^2} +  \\frac{z^2}{1+z^2} \\, \\frac{dg(x;\\theta)}{dz},\n",
    "\\end{align}\n",
    "where $x = (z, u_0, d_0, \\alpha)$.\n",
    "Note this ansatz satisfies $u \\rightarrow g$ and $du/dz \\rightarrow dg/dz$ as $z \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Solution\" class inherits the properties of nn.Module\n",
    "class Solution(nn.Module):\n",
    "    \n",
    "    def __init__(self, NN, scale):\n",
    "\n",
    "        # remember to initialize base (that is, parent) class\n",
    "        super().__init__()\n",
    "\n",
    "        # cache neural network and send to computational device\n",
    "        self.g = NN.to(device)  \n",
    "        \n",
    "        # \"scale\" is needed to ensure that the inputs to the neural network \n",
    "        # are of order 1. \n",
    "        # unsqueeze(dim=0) does the following to the shape of the tensor: (3) => (1, 3).\n",
    "        # use register_buffer to tell PyTorch to save \"scale\" when the NN \n",
    "        self.register_buffer('zscale', torch.tensor(scale[0]).unsqueeze(dim=0))\n",
    "        self.register_buffer('xscale', torch.tensor(scale[1:]).unsqueeze(dim=0))\n",
    "\n",
    "    def save(self, dictfile):\n",
    "        # save parameters of neural network\n",
    "        torch.save(self.g.state_dict(), dictfile)\n",
    "\n",
    "    def load(self, dictfile):\n",
    "        # load parameters of neural network\n",
    "        self.g.eval()\n",
    "        self.g.load_state_dict(torch.load(dictfile))\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        # z  = omega_0 * t, \n",
    "        # u0 = u(0) and \n",
    "        # d0 = du(0)/dz\n",
    "        u0, d0 = x[:, 0].view(-1, 1), x[:, 1].view(-1, 1)\n",
    "\n",
    "        # compute ansatz\n",
    "        zz = z**2\n",
    "\n",
    "        z_scaled = z / self.zscale\n",
    "        x_scaled = x / self.xscale\n",
    "        \n",
    "        u  = (u0 + z * d0 + zz * self.g(z_scaled, x_scaled)) / (1+zz)\n",
    "\n",
    "        return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model objective function\n",
    "\\begin{align}\n",
    "    \\frac{d^2u}{dz^2} + 2 \\alpha \\frac{du}{dz} + u & = 0.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(nn.Module):\n",
    "\n",
    "    def __init__(self, solution):\n",
    "        super().__init__()        \n",
    "        self.solution = solution\n",
    "\n",
    "    def eval(self):\n",
    "        self.solution.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.solution.train()\n",
    "\n",
    "    def save(self, paramsfile):\n",
    "        self.solution.save(paramsfile)\n",
    "        \n",
    "    def forward(self, z, x):\n",
    "        assert(len(z.shape) == 2)\n",
    "        assert(len(x.shape) == 2)\n",
    "\n",
    "        u = self.solution(z, x)\n",
    "        \n",
    "        # compute du/dx = [d/dz] u.\n",
    "        #\n",
    "        # Note: autograd.grad computes the gradient of scalars, but since\n",
    "        # u is generally a batch of scalars with shape (batch-size, 1), we need to\n",
    "        # tell autograd.grad to apply the gradient operator to every scalar in the\n",
    "        # batch. We do this by passing autograd.grad a tensor of shape\n",
    "        # (batch-size, 1). It is also necessary to create the computation graph \n",
    "        # for this operation. \n",
    "        \n",
    "        dudz = torch.autograd.grad(u, z, \n",
    "                                   torch.ones_like(u), \n",
    "                                   create_graph=True)[0]\n",
    "\n",
    "        # now compute [d/dz] du/dz, in other words d2u/dz2\n",
    "        \n",
    "        d2udz2 = torch.autograd.grad(dudz, z, \n",
    "                                  torch.ones_like(dudz), \n",
    "                                  create_graph=True)[0]\n",
    "\n",
    "        # pick off alpha and make sure it's also a batch of scalars.\n",
    "        \n",
    "        alpha = x[:, -1].view(-1, 1)\n",
    "\n",
    "        # compute loss function\n",
    "\n",
    "        loss = d2udz2 + 2*alpha*dudz + u\n",
    "\n",
    "        # compute empirical risk (objective) function\n",
    "        R = torch.mean(loss**2)\n",
    "        \n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_random_batch(z, x, batch_size):\n",
    "    indices = torch.randint(0, len(x)-1, size=(batch_size,))\n",
    "    return z[indices], x[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(objective, optimizer, getbatch, \n",
    "          train_z, train_x, \n",
    "          valid_z, valid_x, \n",
    "          batch_size,\n",
    "          number_iterations,\n",
    "          lossfile=LOSS_FILE,\n",
    "          timeleftfile=TIMELEFT_FILE,\n",
    "          paramsfile=PARAMS_FILE,\n",
    "          step=100):\n",
    "\n",
    "    def validate(fcn, z, x):\n",
    "        fcn.eval()\n",
    "        return fcn(z, x)\n",
    "       \n",
    "    n = len(valid_x)\n",
    "\n",
    "    # start saving model parameters after the following number of iterations.\n",
    "    \n",
    "    start_saving = number_iterations // 100\n",
    "    min_avloss   = float('inf')  # initialize minimum average loss\n",
    "\n",
    "    # initialize loss file\n",
    "    # create loss file if it does not exist\n",
    "    if not os.path.exists(lossfile):\n",
    "        open(lossfile, 'w').write('iteration,t_loss,v_loss\\n')  \n",
    "\n",
    "    # get last iteration number from loss file\n",
    "    df = pd.read_csv(lossfile)\n",
    "    if len(df) < 1:\n",
    "        itno = 0\n",
    "    else:\n",
    "        itno = df.iteration.iloc[-1] # get last iteration number\n",
    "\n",
    "    # enter training loop\n",
    "    \n",
    "    timeleft = TimeLeft(number_iterations)\n",
    "    \n",
    "    for ii in range(number_iterations):\n",
    "        \n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        \n",
    "        # set mode to training so that training-specific \n",
    "        # operations such as dropout, etc., are enabled.\n",
    "        \n",
    "        objective.train()\n",
    "    \n",
    "        # get a batch of data \n",
    "        \n",
    "        z, x = getbatch(train_z, train_x, batch_size)     \n",
    "\n",
    "        y = objective(z, x)         # compute objective with current model parameters\n",
    "\n",
    "        y.backward()                # compute gradients \n",
    "        \n",
    "        optimizer.step()            # move one step in average loss landscape\n",
    "\n",
    "        # i'm alive printout\n",
    "        \n",
    "        if (ii % step == 0) and (ii > 0):\n",
    "            \n",
    "            t_loss = validate(objective, \n",
    "                              train_z[:n], train_x[:n]).detach() # detach from comp. graph\n",
    "            \n",
    "            v_loss = validate(objective, valid_z, valid_x).detach()\n",
    "\n",
    "            # update loss file\n",
    "            \n",
    "            open(loss_file, 'a').write(f'{itno:12d},{t_loss:12.8},{v_loss:12.8}\\n')\n",
    "\n",
    "            # save model paramters to file\n",
    "            \n",
    "            if v_loss < min_avloss:\n",
    "                min_avloss = v_loss\n",
    "                if ii > start_saving:\n",
    "                    objective.save(paramsfile)\n",
    "\n",
    "            # update time left file\n",
    "            \n",
    "            line = f'|{itno:12d}|{t_loss:12.8f}|{v_loss:12.8f}|'\n",
    "            timeleft(ii, line)\n",
    "            open(timeleftfile, 'w').write(f'{str(timeleft):s}\\n')\n",
    "\n",
    "            # update iteration number\n",
    "            \n",
    "            itno += step\n",
    "\n",
    "    print()      \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate solution and objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2491,\n",
       " Objective(\n",
       "   (solution): Solution(\n",
       "     (g): FCN(\n",
       "       (znet): Sequential(\n",
       "         (0): Linear(in_features=1, out_features=15, bias=True)\n",
       "         (1): SiLU()\n",
       "         (2): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (3): SiLU()\n",
       "         (4): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (5): SiLU()\n",
       "         (6): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (7): SiLU()\n",
       "         (8): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (9): SiLU()\n",
       "       )\n",
       "       (xnet): Sequential(\n",
       "         (0): Linear(in_features=3, out_features=15, bias=True)\n",
       "         (1): SiLU()\n",
       "         (2): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (3): SiLU()\n",
       "         (4): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (5): SiLU()\n",
       "         (6): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (7): SiLU()\n",
       "         (8): Linear(in_features=15, out_features=15, bias=True)\n",
       "         (9): SiLU()\n",
       "       )\n",
       "       (cnet): Sequential(\n",
       "         (0): Linear(in_features=30, out_features=15, bias=True)\n",
       "         (1): SiLU()\n",
       "         (2): Linear(in_features=15, out_features=1, bias=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn  = FCN(n_input=4, n_hidden=4, n_width=15).to(device)\n",
    "\n",
    "soln = Solution(fcn, SCALE).to(device)\n",
    "\n",
    "objective = Objective(soln).to(device)\n",
    "\n",
    "number_of_parameters(fcn), objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "If $\\texttt{niterations}$ is the number of iterations, $\\texttt{ntrain}$ the training sample size, and $\\texttt{nbatch}$ the batch size, the effective number of epochs, $\\texttt{nepochs}$, is given by\n",
    "$$\\texttt{nepochs} = \\texttt{niterations} \\times \\texttt{nbatch} / \\texttt{ntrain}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = LOSS_FILE\n",
    "timeleft_file = TIMELEFT_FILE\n",
    "params_file = PARAMS_FILE\n",
    "\n",
    "niterations = 200001\n",
    "nbatch = 100 \n",
    "step = 100\n",
    "learning_rate = 2e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(objective.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs: 167\n"
     ]
    }
   ],
   "source": [
    "nepochs = niterations * nbatch / ntrain\n",
    "print(f'number of epochs: {round(nepochs):d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2501|  1.25%|00:00:08/00:11:06| 296.5 it/s|        2400|  0.05179259|  0.05243890|"
     ]
    }
   ],
   "source": [
    "DELETE_LOSS_FILE = True\n",
    "\n",
    "if DELETE_LOSS_FILE:\n",
    "    os.system(f'rm -f {loss_file:s}')\n",
    "\n",
    "solve(objective, optimizer, get_random_batch,\n",
    "            train_Z, train_X, valid_Z, valid_X, nbatch, niterations,\n",
    "            loss_file, timeleft_file, params_file, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(z, x, K=15, \n",
    "             zmin=LOWER[0], zmax=UPPER[0], nz=201, \n",
    "             paramsfile=PARAMS_FILE):\n",
    "\n",
    "    # generate random parameter points\n",
    "\n",
    "    U0, D0, ALPHA = x[:, 0][:K], x[:, 1][:K], x[:, 2][:K] \n",
    "\n",
    "    # load model with smallest average loss (empirical risk) \n",
    "    # value over the validation set\n",
    "    \n",
    "    soln.load(paramsfile)\n",
    "\n",
    "    # loop over parameter points\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, (u0, d0, alpha) in enumerate(zip(U0, D0, ALPHA)):\n",
    "        \n",
    "        print(f'\\r{i:5d}', end='')\n",
    "\n",
    "        # generate equally-spaced points in z; make sure shape is correct\n",
    "        # so that z, u0, d0, alpha can be concatenated correctly\n",
    "        \n",
    "        z = torch.linspace(zmin, zmax, nz).view(-1, 1)\n",
    " \n",
    "        # detach from computation graph and send to CPU\n",
    "\n",
    "        z0 = z.detach().cpu().numpy()\n",
    "        u0 = u0.detach().cpu()\n",
    "        d0 = d0.detach().cpu()\n",
    "        alpha = alpha.detach().cpu()\n",
    "        \n",
    "        exact = exact_solution(z0, u0, d0, alpha)\n",
    "\n",
    "        # compute approximate solution\n",
    "        \n",
    "        x = [u0*torch.ones_like(z),\n",
    "             d0*torch.ones_like(z), \n",
    "             alpha*torch.ones_like(z)]\n",
    "\n",
    "        x = torch.cat(x, dim=1).to(device)\n",
    "\n",
    "        approx = soln(z, x)\n",
    "\n",
    "        # detach from computation graphs, send to CPU \n",
    "        # and convert to numpy array\n",
    " \n",
    "        z = z.squeeze().detach().cpu().numpy()\n",
    "        exact = exact.squeeze().detach().cpu().numpy() \n",
    "        approx = approx.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        # cache results\n",
    "        \n",
    "        data.append([z, exact, approx, u0, d0, alpha])\n",
    "        \n",
    "    print()\n",
    "    return data\n",
    "\n",
    "data = get_data(test_Z, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, \n",
    "                 xmin=LOWER[0],  xmax=UPPER[0],\n",
    "                 filename=PLOT_FILE,\n",
    "                 fgsize=(8, 6), \n",
    "                 ftsize=14):\n",
    "\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    # work out number of columns and number of plots\n",
    "    ncols = 3\n",
    "    nrows = len(data) // ncols\n",
    "    ndata = nrows * ncols\n",
    "\n",
    "    fgsize= (2.8*ncols , 2*nrows)\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=fgsize)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i, (x, exact, approx, u0, d0, alpha) in enumerate(data):\n",
    "\n",
    "        index = i+1\n",
    "        ax  = fig.add_subplot(nrows, ncols, index)\n",
    "        \n",
    "        # setup x-axis\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        \n",
    "        if i > (nrows-1) * ncols-1:\n",
    "            ax.set_xlabel(r'$z$', fontsize=ftsize)\n",
    "        \n",
    "        if i % ncols == 0:\n",
    "            ax.set_ylabel(r'$u(z)$', fontsize=ftsize)\n",
    "\n",
    "        # annotate plot\n",
    "        dy = 0.25\n",
    "        ymin = exact.min()\n",
    "        n = int(np.abs(ymin)/dy)+4\n",
    "        ymin = np.sign(ymin)*n*dy\n",
    "\n",
    "        ymax = exact.max()\n",
    "        n = int(np.abs(ymax)/dy)+2\n",
    "        ymax = np.sign(ymax)*n*dy\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "        xpos = xmin + 0.20 * (xmax-xmin)\n",
    "        ypos = ymin + 0.30 * (ymax-ymin)\n",
    "        ystep= (ymax-ymin)/5\n",
    "        \n",
    "        ax.text(xpos, ypos, r'$u_0, d_0, \\alpha$', fontsize=14); ypos -= ystep\n",
    "        ax.text(xpos, ypos, '%8.3f,%8.3f,%8.3f' % (u0, d0, alpha)); ypos -= ystep\n",
    "\n",
    "        ax.plot(x, exact,  color='blue', linestyle='solid', linewidth=1.0)\n",
    "        ax.plot(x, approx, color='red', linestyle='dashed', linewidth=0.5)\n",
    "        \n",
    "    plt.savefig(filename)\n",
    "    #plt.show() # omit plt.show() to avoid launching window outside notebook\n",
    "\n",
    "plot_results(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
