{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DlGeQP8NkVv"
   },
   "source": [
    "# Tutorial09: Transformer Neural Networks (TNN)\n",
    "> Created Aug. 2024 for the FSU Course: *Machine Learning in Physics* <br>\n",
    "> H. B. Prosper<br>\n",
    "\n",
    "Based on project by former FSU student Alex Judge<br>\n",
    "Florida State University, Spring 2023 (closely follows the Annotated Transformer[1])<br>\n",
    "Updated: July 4, 2023 for Terascale 2023, DESY, Hamburg, Germany<br>\n",
    "Updated: March 31, 2024 HBP: load all data onto computational device\n",
    "Updated: November 19, 2024 HBP: for *Machine Learning in Physics course*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial describes a sequence to sequence (**seq2seq**) neural network, called the **transformer**[1], which can be  used to translate one sequence of tokens to another. The tutorial follows closely the excellent description in the Annotated Transformer[2]. \n",
    "\n",
    "In natural language translation, a word, for example $\\texttt{the}$, or part of a word, for example $\\texttt{ly}$, could be a token. In symbolic mathematics, a token might be a mathematical function, e.g. $\\texttt{sin}$. The set of tokens forms a **vocabulary** from which sequences of tokens are constructed. Typically, the vocabulary of the input sequences (the source) differs from that of the output sequences (the target). This makes sense. If one is translating from English to French, it makes sense to have different vocabularies for the two languages.\n",
    "\n",
    "The seq2seq model\n",
    "consists of three parts:\n",
    "\n",
    "  1. The embedding layers: encodes the tokens and their relative positions within sequences. An input (i.e., source) sequence of tokens is thus mapped to a point cloud in a vector space.\n",
    "  1. The transformer layers[1]: implements the syntactic and semantic analysis.\n",
    "  1. The output layer: computes weights, one for every possible token in the output vocabulary of tokens, which are converted to probabilistic predictions for the next token in the output sequence given the input sequence and the current output sequence. \n",
    "\n",
    "__Tensor Convention__\n",
    "We follow the convention used in the Annotated Transformer[2] in which the batch is the first dimension in all tensors. \n",
    "\n",
    "## Sequence to Sequence Model \n",
    "\n",
    "### Introduction\n",
    "A transformer-based seq2seq model comprises an **encoder** and a **decoder**. The encoder embeds every token in the source sequence $\\boldsymbol{x}$ together with its ordinal value  in a vector space. The vectors are processed with a chain of algorithms called **attention** and the transformed vectors together with the current target sequence $\\boldsymbol{t}$ or current predicted output sequence $\\boldsymbol{y}$ are sent to the decoder, which embeds the targets in the same vector space. The target vectors are likewise processed with a chain of attention algorithms, while the target vectors and those from the encoder are processed with another attention algorithm. Finally, the decoder assigns a weight to every token in the target vocabulary. Using a greedy strategy, one chooses the next output token to be the one with the largest weight, that is, the most probable token. The model is **autoregressive**: the predicted token is appended to the existing predicted output sequence and the model is called again with the same source and the updated output. The procedure repeats until either the maximum output sequence length is reached or the end-of-sequence (EOS) token is predicted as the most probable token.\n",
    "\n",
    "\n",
    "### Attention\n",
    "\n",
    "When we translate from one sequence of symbols to another sequence of symbols, for example from one natural language to another,  the meaning of the sequences is encoded in the symbols, their relative order, and the degree to which a given symbol is related to the other symbols. Consider the phrases \"the white house\" and \"la maison blanche\". In order to obtain a correct translation it is important for the model to encode the fact that \"la\" and \"maison\" are strongly related, while \"the\" and \"house\" are less so. It is also important for the model to encode the strong relationship between \"the\" and \"la\", between \"house\" and \"maison\", and between \"white\" and \"blanche\". That is, the model needs to *pay attention to* grammatical and semantic facts. At least as far as we can tell that's what humans do.\n",
    "\n",
    "The need for the model to pay attention to relevant linguistic facts is the basis of the so-called [attention mechanism](https://nlp.seas.harvard.edu/annotated-transformer/). In the encoding stage, the model associates a vector to every token that tries to capture the strength of a token's relationship to other tokens. Since this association mechanism operates within the same sequence (that is, within the same point cloud in the vector space in which the sequence is embedded) it is referred to as **self attention**. Ideally, self attention will note the fact that \"la\" and \"maison\" are strongly coupled and, ideally, that the relative positions of \"maison\" and \"blanche\" are also strongly coupled as are the relative positions of \"white\" and \"house\". In the decoding stage of the model, in addition to the self attention over the target sequences another attention mechanism should pay attention to the fact that \"the\" and \"la\", \"house\" and \"maison\", and \"white\" and \"blanche\" are strongly coupled. At a minimum, therefore, we expect a successful seq2seq model to model self attention in both the encoding and decoding phases and source to target attention in the decoding phase. The optimal way to implement this is not known, but the transformer model implements an attention mechanism, described next, which empirically appears to be highly effective.\n",
    "\n",
    "\n",
    "### Prediction\n",
    "As noted, the transformer is trained and used *autoregressively*: given source, i.e., input, sequence $\\boldsymbol{x} = x_0, x_1,\\cdots, x_k, x_{k+1}$ of length $k+2$ tokens, where $x_0 \\equiv \\text{<sos>}$ denotes the start of sequence token and $x_{k+1} \\equiv \\text{<eos>}$ denotes the end of sequence token and the current output sequence  $\\boldsymbol{y}_l = y_0, y_1,\\cdots, y_{l-1}$ of length $l$ tokens, the model approximates a discrete conditional probability distribution  over the target vocabulary of size $m$ tokens, \n",
    "\\begin{align}\n",
    "p_{ij} & \\equiv p(y_{ij} | \\boldsymbol{x}, \\boldsymbol{y}_l), \\quad i = 0, \\cdots, l, \\quad j = 0,\\cdots, m-1 .\n",
    "\\end{align}\n",
    "For the next position in the output sequennce, the model computes a probability distribution over the output vocabulary. \n",
    "For a vocabulary of size $m$ and a sequence of size $k$ (omitting the delimeters) every position in the sequence can be filled in $m$ ways. Therefore, there are $m^k$ possible sequences of which we want the most probable. Alas we have a bit of a computational problem. For example, for a sequence of size $k=85$ tokens and a target vocabulary of size $m = 28$ tokens there are $\\sim 10^{123}$ possible sentences. Even at a trillion probability calculations per second an exhaustive search would be an utterly futile undertaking because it would take far longer to complete than the current age of the universe ($\\sim 4 \\times 10^{17}$ s)! Obviously, we have no choice but to use a **heuristic strategy**.\n",
    "\n",
    "The simplest such strategy is the **greedy search** in which we consider only the last predicted probability distribution, that is, $p_{lj}$ and choose the most probable token as the next token at position $l$. \n",
    "A potentially better strategy is **beam search** in which at each prediction stage we keep track of the $n$ most probable sequences so far. At the end we pick the most probable sequence among the $n$.\n",
    "\n",
    "\n",
    "### References\n",
    "  1.  [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "  1. [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34921,
     "status": "ok",
     "timestamp": 1686165379916,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "zAi4I2osNkVw",
    "outputId": "1b7c1fac-eede-4984-948d-891fef7664f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../python')\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lossmonitor as lm\n",
    "\n",
    "from dataloader import DataLoader, number_of_parameters, stringify\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nComputational device: {str(DEVICE):s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1686165474275,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "vy7iBa6DNkVy"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpE9QgwvNkVy"
   },
   "source": [
    "## Read Sequence Data\n",
    "\n",
    "The file **seq2seq_series_2terms.txt** contains (source, target) pairs where the targets are the Taylor series expansions of the corresponding sources up to an error term of ${\\cal O}(x^6)$ and the sources are functions built from one or two terms randomly sampled from the set `{exp, sin, cos, tan, sinh, cosh, tanh}`. Since the source sequences are reasonably simple functions it is possible to train a transformer model to predict their Taylor series expansions in under an hour on a GPU. The more complicated functions in the file **seq2seq_series.txt** require more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "executionInfo": {
     "elapsed": 51324,
     "status": "ok",
     "timestamp": 1686165974828,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "WPWuTZDtNkVy",
    "outputId": "efff2fb7-ba09-4777-96ca-6d2fcbf86356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read sequences\n",
      "     0 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\cosh^{3}{\\left(a x \\right)} + \\tanh{\\left(b x \\right)}$"
      ],
      "text/plain": [
       "cosh(a*x)**3 + tanh(b*x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1 + b x - \\frac{b^{3} x^{3}}{3} + \\frac{2 b^{5} x^{5}}{15} + \\frac{3 a^{2} x^{2}}{2} + \\frac{7 a^{4} x^{4}}{8} + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "1 + b*x - b**3*x**3/3 + 2*b**5*x**5/15 + 3*a**2*x**2/2 + 7*a**4*x**4/8 + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4789 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sin{\\left(b x \\right)} + \\tan{\\left(g x \\right)}$"
      ],
      "text/plain": [
       "sin(b*x) + tan(g*x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\left(b + g\\right) + x^{3} \\left(- \\frac{b^{3}}{6} + \\frac{g^{3}}{3}\\right) + x^{5} \\left(\\frac{b^{5}}{120} + \\frac{2 g^{5}}{15}\\right) + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "x*(b + g) + x**3*(-b**3/6 + g**3/3) + x**5*(b**5/120 + 2*g**5/15) + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9578 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\tan{\\left(h x \\right)} + \\sinh{\\left(g x \\right)}$"
      ],
      "text/plain": [
       "tan(h*x) + sinh(g*x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\left(g + h\\right) + x^{3} \\left(\\frac{g^{3}}{6} + \\frac{h^{3}}{3}\\right) + x^{5} \\left(\\frac{g^{5}}{120} + \\frac{2 h^{5}}{15}\\right) + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "x*(g + h) + x**3*(g**3/6 + h**3/3) + x**5*(g**5/120 + 2*h**5/15) + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "source vocabulary\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'a': 20, 'b': 21, 'c': 22, 'cos': 23, 'cosh': 24, 'd': 25, 'exp': 26, 'f': 27, 'g': 28, 'h': 29, 'm': 30, 'n': 31, 'sin': 32, 'sinh': 33, 'tan': 34, 'tanh': 35, 'x': 36}\n",
      "\n",
      "target vocabulary\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'O(x**6)': 20, 'a': 21, 'b': 22, 'c': 23, 'd': 24, 'f': 25, 'g': 26, 'h': 27, 'm': 28, 'x': 29}\n",
      "\n",
      "tokenize\n",
      " 14000\n",
      " 14000\n",
      "delimit and pad training data\n",
      " 10000\n",
      "delimit and pad validation data\n",
      "  1000\n",
      "delimit test data but do not pad\n",
      "     0\n",
      "avg. source sequence length:       14\n",
      "std. source sequence length:        2\n",
      "     source sequence length:       22\n",
      "     source vocabulary size:       37\n",
      "\n",
      "avg. target sequence length:       63\n",
      "std. target sequence length:       38\n",
      "     target sequence length:       85\n",
      "     target vocabulary size:       30\n",
      "\n",
      "\n",
      "training   data: torch.Size([10072, 22]), torch.Size([10072, 85])\n",
      "validation data: torch.Size([1185, 22]), torch.Size([1185, 85])\n",
      "test data:       593\n"
     ]
    }
   ],
   "source": [
    "def pathname(filename):\n",
    "    return filename\n",
    "    \n",
    "short_tutorial = True\n",
    "\n",
    "DICTFILE = pathname('seq2seq.pth')\n",
    "LOSSFILE = pathname('losses.csv')\n",
    "TIMELEFTFILE = pathname('timeleft.txt')\n",
    "\n",
    "if short_tutorial:\n",
    "    \n",
    "    MAX_SEQ_LEN = 85\n",
    "    BATCH_SIZE  = 32\n",
    "    filename = pathname('../data/seq2seq_series_2terms.txt')\n",
    "\n",
    "    BATCH_SIZE    = 32\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NITERATIONS   = 400000\n",
    "    STEP          =    100\n",
    "    \n",
    "else:\n",
    "    \n",
    "    MAX_SEQ_LEN = 200\n",
    "    BATCH_SIZE  = 64\n",
    "    filename = pathname('../data/seq2seq_series.txt')\n",
    "    \n",
    "    BATCH_SIZE    = 64\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NITERATIONS   = 500000\n",
    "    STEP          =    100\n",
    "\n",
    "FTRAIN = 17\n",
    "FVALID = 2\n",
    "FTEST  = 1\n",
    "\n",
    "delimit= '|'\n",
    "\n",
    "dloader= DataLoader(filename, delimit, \n",
    "                      max_seq_len=MAX_SEQ_LEN, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      ftrain=FTRAIN, \n",
    "                      fvalid=FVALID, \n",
    "                      ftest=FTEST)\n",
    "\n",
    "train_data, valid_data, test_data = dloader.data_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rABspL0wNkVz"
   },
   "source": [
    "## The Model\n",
    "\n",
    "The transformer comprises an **encoder** and **decoder**, each of which consists of one or more processing layers.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder does the following:\n",
    " 1. Each token in the source (input) sequence is encoded as a vector $\\boldsymbol{t}$ in a space of $d =$ **emb_dim** dimensions. A sequence is therefore represented as a point cloud in the vector space.\n",
    " 1. The position of each token is also encoded as a vector $\\boldsymbol{p}$ in a vector space of the same dimension as $\\boldsymbol{t}$. We can think of these vectors $\\boldsymbol{p}$ as residing in the same vector space as the vectors $\\boldsymbol{t}$.  Both the token and position embeddings are trainable.\n",
    " 1. Each token is associated with a third vector: $\\boldsymbol{v} = \\lambda \\boldsymbol{t} + \\boldsymbol{p}$, where the scale factor $\\lambda = \\sqrt{d}$.  In this tutorial, we make $\\lambda$ a tunable parameter.\n",
    "\n",
    "The vectors $\\boldsymbol{v}$ are processed through $N$ *encoder layers*.\n",
    "\n",
    "Since the source sequences are **padded** so that they are all of equal length, a method is needed to ensure that the pad tokens are ignored in all calculations. This is done using **masks**.\n",
    "The source mask, `src_mask`, has value 0 if the token in the source is a `<pad>` token and 1 otherwise. There is also a target mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1686166010835,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "H3iANFmUNkVz"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size,      # vocabulary size (of source)\n",
    "                 emb_dim,         # dimension of token embedding space\n",
    "                 n_layers,        # number of encoding layers\n",
    "                 n_heads,         # number of attention heads per encoding layer\n",
    "                 ff_dim,          # dimension of feed-forward network\n",
    "                 dropout,         # dropout probability\n",
    "                 device,          # computational device\n",
    "                 max_len):        # maximum number of tokens per sequence\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # cache computational device\n",
    "        self.device = device\n",
    "        \n",
    "        # represent each of the 'vocab_size' tokens by a vector \n",
    "        # of size d = emb_dim. nn.Embedding \"learns\" a simple\n",
    "        # lookup table that maps the code for each token in the\n",
    "        #. vocabulary to a vector of size emb_dim.\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # represent the position of each token by a vector of \n",
    "        # size d = emb_dim.\n",
    "        # 'max_len' is the maximum length of a sequence.\n",
    "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        # create 'n_layers' encoding layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(emb_dim, \n",
    "                                                  n_heads, \n",
    "                                                  ff_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        # randomly set to zero weights during training.\n",
    "        # dropout is thought to mitigate over-training\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "        # factor by which to scale token embedding vectors.\n",
    "        # use nn.Parameter to tell PyTorch that this is a \n",
    "        # tunable parameter.\n",
    "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src      : [batch_size, src_len]         (shape of src)\n",
    "        # src_mask : [batch_size, 1, 1, src_len]   (shape of src_mask)\n",
    "        \n",
    "        batch_size, src_len = src.shape\n",
    "  \n",
    "        # ---------------------------------------\n",
    "        # token embedding \n",
    "        # ---------------------------------------\n",
    "        src = self.tok_embedding(src)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # ---------------------------------------\n",
    "        # token position embedding\n",
    "        # ---------------------------------------\n",
    "        # create a row tensor, p, with entries [0, 1,..., src_len-1]\n",
    "        pos = torch.arange(0, src_len)\n",
    "        # pos: [src_len]\n",
    "        \n",
    "        # 1. add a dimension at position 0 (for batch size)\n",
    "        # 2. repeat one instance of p per row 'batch_size' \n",
    "        #    times so that we obtain\n",
    "        # pos = |p|\n",
    "        #       |p|\n",
    "        #        :\n",
    "        #       |p|\n",
    "        # 3. send to computational device\n",
    "        once_per_row = 1\n",
    "        #   3.1 unsqueeze inserts a dimension, here dimension 0 \n",
    "        #       so that pos has shape [1, src_len].\n",
    "        #   3.2 repeat this row of integers batch_size times, \n",
    "        #       once per row\n",
    "        #   3.3 send to computational device\n",
    "        pos = pos.unsqueeze(0).repeat(batch_size, once_per_row).to(self.device)\n",
    "        # pos: [batch_size, src_len]\n",
    "        \n",
    "        pos = self.pos_embedding(pos)\n",
    "        # pos: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # linearly combine token and token position embeddings.\n",
    "        # (perhaps this could be replaced by an MLP?)\n",
    "        src = src * self.scale + pos\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # it is not clear how much this helps\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        # now pass embedded vectors through encoding layers.\n",
    "        # Note: every token in the sequence src is processed \n",
    "        # together.\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask) \n",
    "            # src: [batch_size, src_len, emb_dim]\n",
    "\n",
    "        # return the vectors representing the processed tokens.\n",
    "        # the tensor src will be fed into the decoder along with\n",
    "        # the target tensor.\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE4K943iNkV0"
   },
   "source": [
    "### Encoder Layer\n",
    "\n",
    " 1. Pass the source tensor and its mask to the **multi-head attention** layers.\n",
    " 1. Apply a residual connection and [Layer Normalization](https://arxiv.org/abs/1607.06450). \n",
    " 1. Apply a linear layer.\n",
    " 1. Apply a residual connection and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166016054,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "hGmYBW5PNkV1"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 emb_dim, # token embedding dimension\n",
    "                 n_heads, # number of attention \"heads\"\n",
    "                 ff_dim,  # dimension of feed-forward network\n",
    "                 dropout, # dropout probability\n",
    "                 device): # computational device\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention       = MultiHeadAttention(emb_dim, \n",
    "                                                       n_heads, \n",
    "                                                       dropout, \n",
    "                                                       device)\n",
    "        \n",
    "        self.self_attention_norm  = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.feedforward          = Feedforward(emb_dim, ff_dim, dropout)\n",
    "        \n",
    "        self.feedforward_norm     = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src      : [batch_size, src_len, emb_dim]\n",
    "        # src_mask : [batch_size, 1, 1, src_len] \n",
    "          \n",
    "        # ------------------------------------------\n",
    "        # self attention over embedded source tensor\n",
    "        # ------------------------------------------\n",
    "        # distinguish between src and src_ as the \n",
    "        # former is needed later for a residual connection.\n",
    "        # the output of the self_attention layer are vectors\n",
    "        # that incorporate semantic and syntactic information\n",
    "        # about the input tokens.\n",
    "        #\n",
    "        # Note, for self attention:\n",
    "        #   Q = src\n",
    "        #   K = src\n",
    "        #   V = src\n",
    "        src_ = self.self_attention(src, src, src, src_mask)\n",
    "        # src_: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # how useful is this?\n",
    "        src_ = self.dropout(src_)\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # add a residual connection, followed by\n",
    "        # layer normalization.\n",
    "        # ------------------------------------------\n",
    "        # distinguish between src and src+src_ as the\n",
    "        # former is later needed for another \n",
    "        # residual connection.\n",
    "        src  = self.self_attention_norm(src + src_)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "\n",
    "        # apply a feed-forward network\n",
    "        src_ = self.feedforward(src)\n",
    "        # src_: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        src_ = self.dropout(src_)\n",
    "\n",
    "        # add residual connection and layer normalization\n",
    "        src  = self.feedforward_norm(src + src_)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTKA5UTANkV1"
   },
   "source": [
    "## Multi-Head Attention Layer\n",
    "\n",
    "Attention is the key to the transformer model. In the transformer model attention is defined by the matrix expression\n",
    "\n",
    "\\begin{align}\n",
    "    \\texttt{Attention}(Q, K, V) & = \\texttt{Softmax}\\left(\\frac{Q K^T}{\\sqrt{d}} \\right) V,\n",
    "\\end{align}\n",
    "\n",
    "where $Q$ is called the `query`, $K$ the `key`, $V$ the `value`, and $d =$ **emb_dim** is the dimension of the vectors that represent the tokens. The Google researchers found that it is better to split each vector representing a token into **n_heads** smaller vectors each of size \n",
    "$$\\textrm{\\bf head\\_dim} = d / \\textrm{\\bf n\\_heads}.$$ \n",
    "The integer **n_heads** is the number of so-called **attention heads**. It is claimed, with some justification in the Google paper, that each head pays attention to different aspects of a sequence. It is certainly plausible that this splitting procedure enhances the flexibility of the model, however, at our current level of understanding of how functions with millions of parameters truly work, such claims should nonetheless be taken with a liberal pinch of salt.\n",
    "\n",
    "In self attention, the query, key, and value tensors are derived from the *same* tensor, either the source or target tensor, via separate linear transformations of that tensor (see *Attention Algorithm* below). The coefficients of the linear functions are free parameters to be determined by the training algorithm.  The number of rows in $Q$, $K$, and $V$, namely, **query_len**,  **key_len**, and **value_len**, respectively, is equal to the sequence length **seq_len**. For target/source attention, the query is a linear function of the target tensor while the key and value tensors are linear functions of the source tensor, where, again, the coefficients are free parameters to be fitted during training.\n",
    "\n",
    "We first describe the attention mechanism mathematically and then follow with an algorithmic description that closely follows \n",
    "the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/). It is to be understood that every operation described below is performed for a batch of sequences. Therefore, when we refer to a matrix we really mean a batch of matrices. \n",
    "\n",
    "Because each vector has been split into n_heads vectors of dimension $head_dim$, the calculations below are applied separately to each such vector. At the end, the smaller vectors are coalesced back into vectors of the embedding dimension. First consider the matrix product $Q K^T$ in component form, where summation over repeated indices (the Einstein convention) is implied,\n",
    "\\begin{align}\n",
    "A_{qk} \n",
    "& = Q_{q h} \\, [K^T]_{hk}, \\nonumber\\\\\n",
    "& \\quad q=1,\\cdots, \\text{query\\_len}, \\,\\, h = 1, \\cdots, \\text{head\\_dim}, \\,\\, k = 1, \\cdots, \\text{key\\_len} .\n",
    "\\end{align}\n",
    "When the matrix $A$ is scaled and a softmax function applied elementwise along the key length dimension (here, horizontally) the result is another matrix $W$ whose row elements, by construction, sum to unity. The matrix $W$, which is a matrix of normalized weights, is then multiplied by $V$ to yield the matrix of sub-vectors\n",
    "\\begin{align}\n",
    "    \\text{Attention}_{qh}  \n",
    "    & = W_{qk} V_{kh}, \n",
    "\\end{align}\n",
    "which encodes information about the degree of association between the smaller vectors, each associated with a token.\n",
    "\n",
    "Since tokens are represented by vectors, it is instructive to think of the attention computation geometrically.   Each row, $i$, of $Q$, $K$, and $V$ can be regarded as the vectors $\\boldsymbol{q}_i$, $\\boldsymbol{k}_i$, and $\\boldsymbol{v}_i$, respectively, associated with token $i$, where each vector (really sub-vector) is of dimension head_dim.  Consider, for example, a sequence with **seq_len** = 2. We can write $Q$, $K$, and $V$ as\n",
    "\n",
    "\\begin{align}\n",
    "Q & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right], \\\\\n",
    "K & = \\left[\\begin{matrix} \\boldsymbol{k}_1 \\\\ \\boldsymbol{k}_2 \\end{matrix}\\right], \\text{ and} \\\\\n",
    "V & = \\left[\\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\end{matrix}\\right] ,\n",
    "\\end{align}\n",
    "\n",
    "and $A = Q K^T$ as the outer product matrix\n",
    "\n",
    "\\begin{align}\n",
    "A & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right] \n",
    "\\left[\\begin{matrix} \\boldsymbol{k}_1 & \\boldsymbol{k}_2 \\end{matrix}\\right] ,\n",
    "\\nonumber\\\\\n",
    "& = \\left[\n",
    "\\begin{matrix} \n",
    "\\boldsymbol{q}_1\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_1\\cdot \\boldsymbol{k}_2 \\\\ \n",
    "\\boldsymbol{q}_2\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_2\\cdot \\boldsymbol{k}_2\n",
    "\\end{matrix}\n",
    "\\right] .\n",
    "\\end{align}\n",
    "\n",
    "The matrix $A$ can be interpreted as a measure of the degree to which the $\\boldsymbol{q}$ and $\\boldsymbol{k}$ vectors are aligned. Presumably, the more aligned the two vectors the stronger the relationship between the  tokens they represent. Because of the use of the dot product, the degree of alignment depends both on the angle between the vectors as well as on their magnitudes. Consequently, two vectors can be more strongly aligned than a vector's alignment with itself! \n",
    "\n",
    "After the scaling and softmax operations on $A$, tokens 1 and 2 become associated with vectors $\\boldsymbol{w}_1 =  (w_{11}, w_{12})$ and $\\boldsymbol{w}_2 =  (w_{21}, w_{22})$, respectively, where\n",
    "\\begin{align}\n",
    "    w_{ij} & = \\frac{\\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_j \\, / \\, \\sqrt{d}\\right)}\n",
    "    {\\sum_{k = 1}^2 \\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_k \\, / \\, \\sqrt{d}\\right)} .\n",
    "\\end{align}\n",
    "\n",
    "These (weight) vectors lie in the line segment $[\\boldsymbol{p}_1, \\boldsymbol{p}_2]$ depicted in the figure below. The line segment is a simplex (here, a 1-simplex) that is embedded in a vector space of dimension **seq_len**.  In this vector space, tokens 1 and 2 are represented by the orthogonal unit vectors $\\boldsymbol{u}_1$ and $\\boldsymbol{u}_2$, respectively. For a sequence of length $n$, the vectors $\\boldsymbol{w}_i$, $i = 1,\\cdots, n$ lie in the $(n-1)$-simplex and, again, each coordinate unit vector $\\boldsymbol{u}_i$ represents a token.  \n",
    "<img src=\"./simplex.png\" align=\"left\" width=\"250px\"/>\n",
    "The vector for token, $i$, is the weighted average \n",
    "\\begin{align}\n",
    "    \\text{Attention}_i & = w_{i1}  \\boldsymbol{v}_1 + w_{i2} \\boldsymbol{v}_2\n",
    "\\end{align}\n",
    "of the so-called value vectors $\\boldsymbol{v}_1$ and $\\boldsymbol{v}_2$. In both self attention and (source to target) attention, the value vectors are source token vectors.\n",
    "\n",
    "The upshot of this construction is that vectors representing the tokens are moved about in the embedding space in complex ways in accordance with the attention operation in such a way that their relative positions within that space encodes information about the degree and nature of the association between the tokens. \n",
    "<br clear=\"left\"/>\n",
    "\n",
    "\n",
    "### Attention Algorithm\n",
    "\n",
    "Now we describe the transformer attention mechanism algorithmically, following closely the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), but with some notational changes.\n",
    "\n",
    "#### Step 1\n",
    "As noted, the attention mechanism starts with three tensors, $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$, of shapes **[batch_size, query_len, emb_dim]**, **[batch_size, key_len, emb_dim]**, and **[batch_size, value_len, emb_dim]**, respectively, with **value_len = key_len**. (emb_dim is called hid_dim in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)).  For self attention, $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$ are the same tensor, while for target to source attention $Q_\\text{in}$ is associated with the target tensor and $K_\\text{in}$ and $V_\\text{in}$ with the source tensor.\n",
    "\n",
    "Three trainable linear layers, $f_Q$, $f_K$, $f_V$, are defined, each of shape *[emb_dim, emb_dim]**, which yield the so-called `query`, `key`, and `value` tensors\n",
    "\\begin{align}\n",
    "    Q & = f_Q(\\boldsymbol{Q_\\text{in}}),\\\\\n",
    "    K & = f_K(\\boldsymbol{K_\\text{in}}), \\text{ and}\\\\\n",
    "    V & = f_V(\\boldsymbol{V_\\text{in}}).\n",
    "\\end{align}\n",
    "Each tensor $Q$, $K$, and $V$ is the same shape as $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$, respectively. \n",
    "\n",
    "#### Step 2\n",
    "Tensors $Q$, $K$, and $V$ are reshaped by first splitting the embedding dimension, **emb_dim**, into **n_heads** blocks of size **head_dim = emb_dim / n_heads** so that their shapes become **[batch_size, -1, n_heads, head_dim]**, where the **-1** pertains to **query_len**, **key_len**, or **value_len**, whose value is determined at runtime. By the way, this shows why the source (and target) masks have shape [batch_size, 1, 1, emb_dim] as will become clear below.\n",
    "\n",
    "#### Step 3\n",
    "Dimensions 1 and 2 of the tensors $Q$, $K$, and $V$ are permuted (`Tensor.permute(0, 2, 1, 3)`) so that we now have **[batch_size, n_heads, -1, head_dim]**. Tensor $K$ is further permuted (`Tensor.permute(0, 1, 3, 2)`) to shape **[batch_size, n_heads, head_dim, -1]** so that it represents $K^T$.\n",
    "\n",
    "#### Step 4\n",
    "Tensor $A = Q K^T$ is computed using `torch.matmul(Q, K^T)`, scaled by $1 \\, / \\, \\sqrt{d}$, and a softmax is applied to the last dimension of $A$, that is, the key/value length dimension, yielding the tensor $W$ of shape **[batch_size, n_heads, query_len, key_len]**.\n",
    "\n",
    "#### Step 5\n",
    "$\\text{Attention} = W V$ is computed, yielding a tensor of shape \n",
    "**[batch_size, n_heads, query_len, head_dim]**.\n",
    "\n",
    "#### Step 6\n",
    "The n_heads and query_len dimensions of `Attention` are transposed (`Tensor.permute(0, 2, 1, 3)`) to shape **[batch_size, query_len, n_heads, head_dim]** and forced to be contiguous in memory (`contiguous()`).\n",
    "\n",
    "#### Step 7\n",
    "The **n_heads** vectors of dimension and **head_dim** are concatenated using `Attention.view(batch_size, seq_len, emb_dim)` to merge the attention heads into a single `MultiHeadAttention` tensor.\n",
    "\n",
    "#### Step 8\n",
    "Finally, the merged `MultiHeadAttention` tensor is pushed through a trainable linear layer of shape **[emb_dim, emb_dim]** to output a tensor of shape **[batch_size, seq_len, emb_dim]**. \n",
    "\n",
    "### Comments\n",
    "As noted above, it is claimed that the  algorithm above captures the notion of \"paying attention to\" token-token associations both within the same sequence and across sequences and that each attention head \"pays attention to\" a different aspect of the sequences. All such claims should be taken with a pinch of salt for at least two reasons.\n",
    "First, it is not at all obvious that this computation aligns with our intuitive understanding of  that notion and, second, the computation is nested through multiple attention layers. Therefore, whatever the attention layers are doing, it is distributed over multiple layers in a highly non-linear, non-local, way. \n",
    "\n",
    "It is, however, undeniable that the transformer has yielded amazing results. Therefore, we are forced to concede that, in practice,  whatever is going on in the attention layers the algorithm works wonders!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166016625,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "NUD-6rRKNkV1"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_heads, dropout, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # emb_dim must be a multiple of n_heads\n",
    "        assert emb_dim % n_heads == 0\n",
    "        \n",
    "        self.emb_dim  = emb_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        \n",
    "        self.linear_Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_V = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_O = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
    " \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # query  : [batch_size, query_len, emb_dim]\n",
    "        # key    : [batch_size, key_len,   emb_dim]\n",
    "        # value  : [batch_size, value_len, emb_dim]\n",
    "        \n",
    "        batch_size, _, emb_dim = query.shape\n",
    "        assert emb_dim == self.emb_dim\n",
    "        \n",
    "        Q = self.linear_Q(query)\n",
    "        # Q: [batch_size, query_len, emb_dim]\n",
    "        \n",
    "        K = self.linear_K(key)\n",
    "        # K: [batch_size, key_len,   emb_dim]\n",
    "        \n",
    "        V = self.linear_V(value)\n",
    "        # V: [batch_size, value_len, emb_dim]\n",
    "        \n",
    "        # split vectors of size emb_dim into 'n_heads' vectors each\n",
    "        # of size 'head_dim' and then permute dimensions 1 and 2\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        \n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # K: [batch_size, n_heads, key_len,   head_dim]        \n",
    "        \n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "          \n",
    "        # transpose K (by permuting key_len and head_dim), then\n",
    "        # compute QK^T/scale\n",
    "        A = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        # A: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # apply (optional) map to ensure that pad tokens do \n",
    "        # not contribute to the attention calculation.\n",
    "        if mask is not None:\n",
    "            A = A.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # apply softmax to the last dimension (i.e, to key len)\n",
    "        # WARNING: W is referred to as 'attention' in Annotated Transformer!\n",
    "        W = torch.softmax(A, dim=-1)     \n",
    "        # W: [batch_size, n_heads, query_len, key_len]\n",
    "        \n",
    "        # not sure why dropout is useful here\n",
    "        W = self.dropout(W)\n",
    "        \n",
    "        # compute attention: (QK^T/scale)V\n",
    "        attention = torch.matmul(W, V)\n",
    "        # attention: [batch_size, n_heads, query_len, head_dim]\n",
    "        \n",
    "        # permute n heads and query len and make sure the tensor \n",
    "        # is contiguous in memory...\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous()\n",
    "        # attention: [batch_size, query_len, n_heads, head_dim]\n",
    "        \n",
    "        # ... and concatenate the n heads into a single multi-head \n",
    "        # attention tensor.\n",
    "        # if attention is being applied to source sequences, then\n",
    "        # query_len = src_len. If applied to output sequences, then\n",
    "        # query_len = trg_len.\n",
    "        attention = attention.view(batch_size, -1, self.emb_dim)\n",
    "        # attention: [batch_size, query_len, emb_dim]\n",
    "        \n",
    "        output = self.linear_O(attention)\n",
    "        # output: [batch_size, query_len, emb_dim]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oacePmrSNkV2"
   },
   "source": [
    "### Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1686166021797,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Uzlh65gaNkV2"
   },
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, ff_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(emb_dim, ff_dim)\n",
    "        \n",
    "        self.linear_2 = nn.Linear(ff_dim, emb_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # x: [batch_size, seq_len, ff_dim]\n",
    "        \n",
    "        x = self.linear_2(x)\n",
    "        # x: [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlidARf5NkV2"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder takes the encoded representation of the source sequence, which is represented by a point cloud in a vector space of dimension **emb_dim**, together with the target sequence, or the current predicted output sequence, and computes weights over the target vocabulary which can be converted to a probability distribution over the target vocabulary for the next output token. \n",
    "\n",
    "The decoder has two multi-head attention layers: a *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value. The mask, as discussed below, is in addition to the one that masks out the pad tokens.\n",
    "\n",
    "**Note**: In PyTorch, the softmax operation, which converts the output weights to probabilities, is contained within the loss function, so the decoder does not have a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166025870,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "dMQz3fxONkV3"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size,   # size of target vocabulary\n",
    "                 emb_dim,      # dimension of embedding vector space\n",
    "                 n_layers,     # number of decoder layers\n",
    "                 n_heads,      # number of masked attention heads\n",
    "                 ff_dim,       # hidden dimension of feed-forward network\n",
    "                 dropout,      # weight dropout probability\n",
    "                 device,       # computational device\n",
    "                 max_len):     # maximum output sequence length\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        self.layers  = nn.ModuleList([DecoderLayer(emb_dim, \n",
    "                                                   n_heads, \n",
    "                                                   ff_dim, \n",
    "                                                   dropout, \n",
    "                                                   device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.linear  = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
    " \n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        # trg      : [batch_size, trg_len]\n",
    "        # src      : [batch_size, src_len, emb_dim]\n",
    "        # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
    "        # src_mask : [batch_size, 1, 1, src_len]\n",
    "                \n",
    "        batch_size, trg_len = trg.shape\n",
    "        \n",
    "        # see Encoder for comments\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)       \n",
    "        # pos: [batch_size, trg_len]\n",
    "            \n",
    "        trg = self.tok_embedding(trg) * self.scale + self.pos_embedding(pos)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg = self.dropout(trg)\n",
    "        \n",
    "        # send the *same* source tensor to every decoding layer. however,\n",
    "        # analogously to the Encoder, in the Decoder the target tensor is \n",
    "        # processed through a sequence of layers. \n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "            # trg: [batch_size, trg_len, emb_dim]\n",
    "\n",
    "        # for each output token, output 'vocab_size' weights,\n",
    "        # which later will be converted to probabilities.\n",
    "        output = self.linear(trg)\n",
    "        # output: [batch_size, trg_len, vocab_size]\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBhZAhu4NkV3"
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "The decoder layer has two multi-head attention layers, `self_attention` and `attention`. The former applies the attention algorithm to the target sequences, while the latter applies the algorithm between the target and source sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1686166028973,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JgYbJN_6NkV3"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 n_heads, \n",
    "                 ff_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # attention within the target sequences\n",
    "        self.self_attention      = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
    "        \n",
    "        self.self_attention_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # attention between the source and target sequences\n",
    "        self.attention           = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
    "        \n",
    "        self.attention_norm      = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.feedforward         = Feedforward(emb_dim, ff_dim, dropout)\n",
    "        \n",
    "        self.feedforward_norm    = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        # trg      : [batch size, trg len, emb dim]\n",
    "        # src      : [batch size, src len, emb dim]\n",
    "        # trg_mask : [batch size, 1, trg len, trg len]\n",
    "        # src_mask : [batch size, 1, 1, src len]\n",
    "        \n",
    "        # compute attention over embedded target sequences.\n",
    "        # distinguish between trg and trg_, since the former \n",
    "        # is needed later for residual connections.\n",
    "        #                          Q    K    V             \n",
    "        trg_ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.dropout(trg_)\n",
    "        \n",
    "        # residual connection and layer norm\n",
    "        trg  = self.self_attention_norm(trg + trg_)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "            \n",
    "        # target to source attention\n",
    "        #                     Q    K    V\n",
    "        trg_ = self.attention(trg, src, src, src_mask)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.dropout(trg_)\n",
    "        \n",
    "        # residual connection and layer norm\n",
    "        trg  = self.attention_norm(trg + trg_)      \n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.feedforward(trg)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg = self.dropout(trg)\n",
    "        \n",
    "        # residual and layer norm\n",
    "        trg  = self.feedforward_norm(trg + trg_)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B7SgrOYNkV3"
   },
   "source": [
    "## Seq2Seq Model\n",
    "\n",
    "The `Seq2Seq` model encapsulates the encoder and decoder and handles the creation of the source and target masks.\n",
    "\n",
    "The source mask, as described above, masks out `<pad>` tokens: the mask is 0 where the token is  a `<pad>` token and 1 otherwise. The mask is then unsqueezed so it can be correctly broadcast to tensors of shape **[batch_size, n_heads, seq_len, seq_len]**, which appear in the multi-head attention calculation.\n",
    "The target mask also includes a mask for the `<pad>` tokens.\n",
    "\n",
    "Consider a target sequence $\\text{<sos>}, t_1,\\cdots, t_{k}, \\text{<eos>}$ of length $k+2$ constructed with tokens, $t_i$, from the target vocabulary and delimited by the special tokens $t_0 \\equiv \\text{<sos>}$ and $t_{k+1} \\equiv \\text{<eos>}$, the start-of-sequence and end-of-sequence tokens, respectively. Ideally, we would like the ability to test the quality of all sub-sequences *simultaneously*. For example, given sub-sequences $\\text{<sos>}$ and  $\\text{<sos>}, t_1$ we would like the ability to check simultaneously the prediction $\\text{<sos>} \\rightarrow y_1 \\sim t_1$ and the prediction $\\text{<sos>}, t_1 \\rightarrow y_2 \\sim t_2$ and so on, where $y_1$ and $y_2$ are the model predictions for the next tokens for the sub-sequences and $t_1$ and $t_2$ are the associated targets. We use the symbol $\\sim$ to mean \"compare with\". In the transformer this is achieved with a simple, clever, trick: the so-called *subsequent* mask, `trg_sub_mask`. This mask is created using  the function $\\texttt{torch.tril}$, which creates a lower diagonal square matrix where the elements above the diagonal are zero and the elements below the diagonal are one. For example, for a target sequence $\\boldsymbol{y} = \\text{<sos>}, t_1, t_2, t_{3}, \\text{<eos>}$ comprising 5 tokens  the `trg_sub_mask` looks like this:\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "When the mask is applied to a target sequence, the former defines which target sub-sequence is available for predicting the next token. For example, the first token of the target sequence has the mask **[1, 0, 0, 0, 0]**. Therefore, along with the entire source sequence, $\\boldsymbol{x}$, only the `<sos>` token of the target sequence is available for prediction of the next token.  During training, suppose that the model predicts the token $y_1$. This prediction is compared with the second target token, that is, the token $t_1$. \n",
    "\n",
    "At the same time, the second token of the target sequence has the mask **[1, 1, 0, 0, 0]**.  Therefore,\n",
    "along with the entire source sequence, $\\boldsymbol{x}$, only the target tokens `<sos>` and $t_1$ are available during training for prediction of the 3rd token, $y_2$, which is to be compared with the desired target token $t_2$, and similarly for the remaining target tokens. \n",
    "In this way, all sub-sequences of a target sequence are processed simultaneously rather than one token at a time.\n",
    "By using a subsequent mask during training the following predictions and comparisons,\n",
    "\\begin{align}\n",
    "  \\boldsymbol{x}, \\text{<sos>} & \\rightarrow y_1 \\sim t_1,\\\\\n",
    "  \\boldsymbol{x}, \\text{<sos>}, t_1  & \\rightarrow y_2 \\sim t_2, \\\\\n",
    "        : & : \\\\\n",
    "  \\boldsymbol{x}, \\text{<sos>}, t_1,\\cdots, t_{k}  & \\rightarrow y_{k+1} \\sim \\text{<eos>} ,\n",
    "\\end{align}\n",
    "can be performed simultaneously, \n",
    "with a loss computed for each comparison $y_i \\sim t_i$. Notice that this requires the input sequences into the decoder be stripped of the end-of-sequence tokens while the target sequences must be stripped of the start-of-sequence tokens before the losses are computed.\n",
    "    \n",
    "During evaluation, the model is used autoregressively: the source sequence $\\boldsymbol{x}$ and target $\\text{<sos>}$ is used to predict $y_1$, then the source sequence and sequence $\\text{<sos>}, y_1$ is used to predict $y_2$ and so on until either $\\text{<eos>}$ is predicted or the maximum target sequence is reached, whichever comes first.\n",
    "\n",
    "The target mask is the logical and of the pad and subsequent masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1686166033948,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Ifek5BPqNkV4"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad, \n",
    "                 trg_pad, \n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad = src_pad\n",
    "        self.trg_pad = trg_pad\n",
    "        self.device  = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg: [batch size, trg len]\n",
    "        \n",
    "        _, trg_len = trg.shape\n",
    "            \n",
    "        trg_pad_mask = (trg != self.trg_pad).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), \n",
    "                                             device=self.device)).bool()\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "            \n",
    "        # logical AND of the two masks\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "        \n",
    "        src      = self.encoder(src, src_mask)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "                \n",
    "        # the decoder will encode the target sequences \n",
    "        # before applying the attention layers.\n",
    "        output   = self.decoder(trg, src, trg_mask, src_mask)\n",
    "        # output: [batch_size, trg_len, trg_vocab_size]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def save(self, paramsfile):\n",
    "        # save parameters of neural network\n",
    "        torch.save(self.state_dict(), paramsfile)\n",
    "\n",
    "    def load(self, paramsfile):\n",
    "        # load parameters of neural network\n",
    "        self.eval()\n",
    "        self.load_state_dict(torch.load(paramsfile, \n",
    "                                        map_location=torch.device(self.device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haNBPOjUNkV4"
   },
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but it is able to be trained on a single GPU in less than an hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB4xmAIVNkV4"
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "As noted in Section *Seq2Seq Model* above, given the entire source sequence, $\\boldsymbol{x}$, and all target sub-sequences, the model is designed to predict the next token for every target sub-sequence. Consider, again, a target sequence of size $k = 5$. Since we want the model to predict `<eos>`, we slice off the `<eos>` token from the end of the targets,\n",
    "\\begin{align}\n",
    "\\text{trg} &= [\\text{<sos>}, t_1, t_2, t_3, \\text{<eos>}]\\\\\n",
    "\\text{trg[:-1]} &= [\\text{<sos>}, t_1, t_2, t_3],\n",
    "\\end{align}\n",
    "where the $t_i$ denote target sequence tokens other than `<sos>` and `<eos>`. The sliced targets are fed into the model, which predicts simultaneously the next token for each sub-sequence. If the model is flexible enough and properly trained, the prediction from the last sub-sequence, $[\\text{<sos>}, t_1, t_2, t_3]$, will be `<eos>`. The loss is computed using the original targer tensor with the `<sos>` token stripped away,\n",
    "\\begin{align}\n",
    "\\text{output} &= [y_1, y_2, y_3, \\text{<eos>}]\\\\\n",
    "\\text{trg[1:]} &= [t_1, t_2, t_3, \\text{<eos>}] .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1686169911360,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "HTJs53_3NkV4"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader,\n",
    "          niterations, \n",
    "          batch_size, \n",
    "          pad_code,\n",
    "          lossfile=LOSSFILE,\n",
    "          timeleftfile=TIMELEFTFILE,\n",
    "          paramsfile=DICTFILE,\n",
    "          valid_size=256,\n",
    "          step=100):\n",
    "    \n",
    "    train_data, valid_data, _ = dataloader.data_splits()\n",
    "    dataloader.set_batch_size(batch_size)\n",
    "\n",
    "    def compute_loss(x, t):\n",
    "        # x: [batch_size, src_seq_len]\n",
    "        # t: [batch_size, trg_seq_len]\n",
    "       \n",
    "        # slice off <eos> token from all targets.\n",
    "        # for every sub-sequence, the model computes\n",
    "        # a series of trg_vocab_size weights (logits)\n",
    "        # simultaneously\n",
    "        y = model(x, t[:,:-1])\n",
    "        # [batch_size, trg_seq_len, trg_vocab_size]\n",
    "\n",
    "        # reshape y for the subsequent loss calculations\n",
    "        trg_vocab_size = y.shape[-1]\n",
    "        y_out = y.reshape(-1, trg_vocab_size)\n",
    "        # [batch_size * tgt_seq_len, tgt_vocab_size]\n",
    "        \n",
    "        # for each sub-sequence, the model predicts y1, y2,...,yk, yk+1,\n",
    "        # where, one hopes, yk+1 = <eos>. therefoe, we need to\n",
    "        # strip away the <sos> token from the targets to arrive\n",
    "        # at the target sequence, t1, t2, ..., <eos> before we\n",
    "        # compute losses.\n",
    "        t_out = t[:, 1:].reshape(-1)\n",
    "        # [batch_size * tgt_seq_len]\n",
    "        \n",
    "        loss = loss_fn(y_out, t_out).mean()\n",
    "\n",
    "        return loss\n",
    "  \n",
    "    def validate(ii):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():  # no need to compute gradients wrt. to x, t\n",
    "                \n",
    "            x, t   = dataloader.get_batch(train_data, ii, batch_size=valid_size)\n",
    "            t_loss = compute_loss(x, t).item()\n",
    "                \n",
    "            x, t   = dataloader.get_batch(valid_data, ii, batch_size=valid_size)\n",
    "            v_loss = compute_loss(x, t).item()\n",
    "            \n",
    "        return t_loss, v_loss\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # enter training loop\n",
    "    # ------------------------------------------\n",
    "\n",
    "    losswriter = lm.LossWriter(model, niterations, \n",
    "                               lossfile, timeleftfile, paramsfile, \n",
    "                               step)\n",
    "    \n",
    "    for ii in range(niterations):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        src, tgt = dataloader.get_batch(train_data, ii)\n",
    "        \n",
    "        loss = compute_loss(src, tgt)\n",
    "\n",
    "        optimizer.zero_grad()     # zero gradients\n",
    "        \n",
    "        loss.backward()           # compute gradients\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()          # make a single step in average loss\n",
    "\n",
    "        if ii % step == 0:\n",
    "        \n",
    "            t_loss, v_loss = validate(ii)\n",
    "\n",
    "            losswriter(ii, t_loss, v_loss)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SRC_LEN= dloader.SRC_SEQ_LEN\n",
    "INPUT_DIM  = dloader.SRC_VOCAB_SIZE\n",
    "\n",
    "MAX_TRG_LEN= dloader.TGT_SEQ_LEN\n",
    "OUTPUT_DIM = dloader.TGT_VOCAB_SIZE\n",
    "\n",
    "if short_tutorial:\n",
    "    EMB_DIM    = 64    # dimension of embedding vector space\n",
    "    ENC_LAYERS = 2     # number of encoder layers\n",
    "    ENC_HEADS  = 8     # number of attention heads\n",
    "    ENC_FF_DIM = 128   # \"hidden\" dimension of feed-forward network\n",
    "    ENC_DROPOUT= 0.1\n",
    "    \n",
    "    DEC_LAYERS = 2     # number of decoder layers\n",
    "    DEC_HEADS  = 8     # number od decoder heads\n",
    "    DEC_FF_DIM = 128\n",
    "    DEC_DROPOUT= 0.1\n",
    "else:\n",
    "    EMB_DIM    = 200   # dimension of embedding vector space\n",
    "    ENC_LAYERS = 4     # number of encoder layers\n",
    "    ENC_HEADS  = 8\n",
    "    ENC_FF_DIM = 1024  # \"hidden\" dimension of feed-forward network\n",
    "    ENC_DROPOUT= 0.1\n",
    "    \n",
    "    DEC_LAYERS = 4\n",
    "    DEC_HEADS  = 8\n",
    "    DEC_FF_DIM = 1024\n",
    "    DEC_DROPOUT= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1686170422923,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "i3tXx6bcNkV4",
    "outputId": "bbd603f0-2858-48c7-8d7d-3e854c54350e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(37, 64)\n",
      "    (pos_embedding): Embedding(22, 64)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feedforward): Feedforward(\n",
      "          (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feedforward_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(30, 64)\n",
      "    (pos_embedding): Embedding(85, 64)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x DecoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feedforward): Feedforward(\n",
      "          (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feedforward_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=30, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 180,518 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(INPUT_DIM, \n",
    "              EMB_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_FF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              DEVICE, \n",
    "              MAX_SRC_LEN)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              EMB_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_FF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              DEVICE, \n",
    "              MAX_TRG_LEN)\n",
    "\n",
    "PAD_CODE = dloader.PAD\n",
    "SOS_CODE = dloader.SOS\n",
    "EOS_CODE = dloader.EOS\n",
    "\n",
    "model = Seq2Seq(enc, dec, PAD_CODE, PAD_CODE, DEVICE).to(DEVICE)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(f'The model has {number_of_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "executionInfo": {
     "elapsed": 2491043,
     "status": "ok",
     "timestamp": 1686172942222,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "dSKnr-o3NkV4",
    "outputId": "a3054dd4-7a16-4062-c329-4513e266a453"
   },
   "outputs": [],
   "source": [
    "LOAD  = False\n",
    "TRAIN = True\n",
    "DELETE_LOSS_FILE = True\n",
    "\n",
    "if LOAD:\n",
    "    # load best model\n",
    "    model.load(DICTFILE)\n",
    "\n",
    "if DELETE_LOSS_FILE:\n",
    "    os.system(f'rm -rf {LOSSFILE:s}')\n",
    "    \n",
    "if TRAIN:\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_CODE)\n",
    "    \n",
    "    train(model, optimizer, criterion, dloader,\n",
    "          niterations=NITERATIONS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          pad_code=PAD_CODE, \n",
    "          lossfile=LOSSFILE,\n",
    "          timeleftfile=TIMELEFTFILE,\n",
    "          paramsfile=DICTFILE,\n",
    "          step=STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFWbhNtNkV4"
   },
   "source": [
    "## Testing Model\n",
    "\n",
    "The test data are already tokenized, coded, and bracketed with the `<sos>` and `<eos>` codes. The translation steps are as follows:\n",
    "\n",
    "  1. convert the coded source tokens, `src`, to the tensor, `src_`, and add a batch dimension to it (at dimension 0) so that the source is of the correct shape, namely, `[batch_size, src_len]`, but with `batch_size = 1`;\n",
    "  1. create the source mask `src_mask` to mask out pad tokens;\n",
    "  1. feed the source `src_` and its mask `src_mask` into the encoder;\n",
    "  1. for the predicted output sequence, create a list initialized with the `<sos>` token;\n",
    "  1. *repeat* steps `A` to `E` below until the model predicts the `<eos>` token or the maximum output length is reached:\n",
    "     1. convert the current output list `trg` into the tensor `trg_` and add a batch dimension at dimension 0 so that like the source tensor, the output tensor will have the shape `[batch_size, src_len]` with `batch_size = 1`;\n",
    "     1. create the target mask `trg_mask` to mask out pad tokens;\n",
    "     1. feed the current output `trg_`, encoder output `src_`, and the source and target masks into the decoder;\n",
    "     1. get the predicted token from the decoder;\n",
    "     1. add the predicted token to the current output list;\n",
    "  1. convert the output sequence from codes to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1686168194747,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "y3-IQ2AdNkV4"
   },
   "outputs": [],
   "source": [
    "def translate(src, model, \n",
    "              max_len=256, \n",
    "              sos=SOS_CODE, \n",
    "              device=DEVICE):\n",
    "    \n",
    "    def execute(trg, src_, src_mask):\n",
    "        \n",
    "        trg_ = torch.tensor(trg).unsqueeze(0).to(device)\n",
    "        # trg_: [batch_size, trg_len], batch_size = 1\n",
    "        \n",
    "        trg_mask = model.make_trg_mask(trg_)\n",
    "        # trg_mask: [batch_size, 1, 1, trg_len]\n",
    "            \n",
    "        with torch.no_grad(): # no need to compute gradients\n",
    "            # defining y0 = <sos>, given the current predicted \n",
    "            # sequence, trg_ = (y0,..yi) and the source \n",
    "            # sequence, src_, compute trg_vocab_size weights \n",
    "            # for the next token for each sub-sequence,\n",
    "            # (y0), (y0, y1), (y0,...,yi).\n",
    "            y = model.decoder(trg_, src_, trg_mask, src_mask)\n",
    "            # y: [batch_size, trg_len, trg_vocab_size]\n",
    "            #\n",
    "            # ...and convert the weights to probabilities by applying\n",
    "            # a softmax to the trg_vocab_size dimension (dim=-1, i.e.,\n",
    "            # horizontally) of the last series of weights (y[:,-1, :])\n",
    "            # Note: y[:, -1, :] is of shape [batch_size, trg_vocab_size]\n",
    "            weights_for_last_token = y[:, -1, :]\n",
    "            probs = torch.softmax(weights_for_last_token, dim=-1)\n",
    "            \n",
    "        # return the 2 token codes with the largest probabilities\n",
    "        token_probs, token_codes = torch.topk(probs, k=2)\n",
    "        token_probs = token_probs.t() # transpose: (trg_len, 2) => (2, trg_len)\n",
    "        token_codes = token_codes.t()\n",
    "\n",
    "        # get most probable code for output token (i.e., the next token)\n",
    "        token_code0 = token_codes[0,-1].item()\n",
    "        \n",
    "        # get next most probable code for output token (i.e., the next token)\n",
    "        token_code1 = token_codes[1,-1].item() \n",
    "        \n",
    "        token_prob0 = token_probs[0,-1].item()\n",
    "        token_prob1 = token_probs[1,-1].item()\n",
    "\n",
    "        return token_code0, token_code1, token_prob0, token_prob1\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Start autoregressive translation\n",
    "    # -------------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    src_ = torch.LongTensor(src).unsqueeze(0).to(device)\n",
    "    # src_: [batch_size, src_len], batch_size = 1\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_)\n",
    "    # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "    # encode (i.e., embed and analyze) source sequence\n",
    "    src_ = model.encoder(src_, src_mask)\n",
    "    # src_: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "    # initialize output sequence with the start-of-sequence token <sos>. \n",
    "    # the decoder takes in the encoded source sequence and for each  \n",
    "    # current output sub-sequence computes weights for the next token.\n",
    "    # the weights for the next token of the last sub-sequence are converted \n",
    "    # to probabilities. \n",
    "    #\n",
    "    # using a greedy strategy, the most probable token is chosen as the \n",
    "    # next token, which is appended to the current output sequence.\n",
    "    # the algorithm repeats and stops when either the <eos> token is\n",
    "    # predicted or the maximum output sequence is reached.\n",
    "    trg0 = [sos] \n",
    "    \n",
    "    for i in range(max_len):\n",
    "        \n",
    "        code0, code1, prob0, prob1 = execute(trg0, src_, src_mask)\n",
    "            \n",
    "        trg0.append(code0)\n",
    "            \n",
    "        if code0 == EOS_CODE:\n",
    "            break\n",
    "            \n",
    "    return trg0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150384,
     "status": "ok",
     "timestamp": 1686173218763,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "fFSdM2NvNkV5",
    "outputId": "f4fa4977-7bdc-4959-93b7-933af1237402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     592\taccuracy:    0.948 +/- 0.009\n"
     ]
    }
   ],
   "source": [
    "srcs, tgts = dloader.test_data\n",
    "MAX_LEN    = dloader.TGT_SEQ_LEN\n",
    "PRINT_MISTAKES = False\n",
    "\n",
    "# load best model\n",
    "model.load(DICTFILE)\n",
    "\n",
    "N = len(srcs)\n",
    "M = 0\n",
    "F = 0.0\n",
    "\n",
    "for i, (src, tgt) in enumerate(zip(srcs[:N], tgts[:N])):   \n",
    "\n",
    "    out  = translate(src, model, \n",
    "                     max_len=MAX_LEN, \n",
    "                     sos=SOS_CODE, \n",
    "                     device=DEVICE)\n",
    "    \n",
    "    # convert sequence of predicted codes to a string (skipping <sos> and <eos> tokens)\n",
    "    out_ = stringify(out[1:-1], dloader.tgt_code2token)\n",
    "\n",
    "    # check prediction\n",
    "    # ----------------\n",
    "    # convert sequence of source codes to a string (skipping <sos> and <eos> tokens)\n",
    "    src_ = stringify(src[1:-1], dloader.src_code2token)\n",
    "    \n",
    "    # convert sequence of target codes to a string (skipping <sos> and <eos> tokens)\n",
    "    tgt_ = stringify(tgt[1:-1], dloader.tgt_code2token)\n",
    "    tgt_ = tgt_.replace('<pad>','') # get rid of pads\n",
    "    \n",
    "    if out_ == tgt_:\n",
    "        M += 1\n",
    "        F = M / (i+1)\n",
    "    else:\n",
    "        if PRINT_MISTAKES:\n",
    "            print()\n",
    "            print(tgt_)\n",
    "            print()\n",
    "            print(out_)\n",
    "            print()\n",
    "            print('-'*91)\n",
    "\n",
    "    print(f'\\r{i:8d}\\taccuracy: {F:8.3f}', end='')\n",
    "\n",
    "dF = math.sqrt(F*(1-F)/N)\n",
    "print(f'\\r{i:8d}\\taccuracy: {F:8.3f} +/- {dF:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1427,
     "status": "ok",
     "timestamp": 1686173496766,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xqsxpTU3NkV6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<loss>:     0.0065\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_from_lists(x, t, model, avloss, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if type(x) == type([]):\n",
    "        x = torch.tensor(x)\n",
    "        t = torch.tensor(t)\n",
    "\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    t = t.unsqueeze(0).to(device)\n",
    "\n",
    "    # slice off EOS token from targets\n",
    "    y = model(x, t[:,:-1])\n",
    "    # [batch_size, trg_seq_len, trg_vocab_size]\n",
    "\n",
    "    trg_vocab_size = y.shape[-1]\n",
    "\n",
    "    y_out = y.reshape(-1, trg_vocab_size)\n",
    "    # [batch_size * tgt_seq_len, tgt_vocab_size]\n",
    "\n",
    "    # slice of SOS token from targets\n",
    "    t_out = t[:, 1:].reshape(-1)\n",
    "    # [batch_size * tgt_seq_len]\n",
    "\n",
    "    loss  = avloss(y_out, t_out).mean().item()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# ---------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_CODE)\n",
    "N = 500\n",
    "aloss = 0.0\n",
    "for i, (src, tgt) in enumerate(zip(srcs[:N], tgts[:N])):\n",
    "    aloss += compute_loss_from_lists(src, tgt, model, criterion, DEVICE)\n",
    "aloss /= N\n",
    "print(f'<loss>: {aloss:10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
