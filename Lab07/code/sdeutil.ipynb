{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33641831-401b-46c3-b64a-49eb33ed8d9f",
   "metadata": {},
   "source": [
    "# Deterministic Diffusion Models (DDM): Utilities\n",
    "> For the Fall 2024 course: *Machine Learning in Physics*<br>\n",
    "> Created: Aug. 14, 2024 Harrison B. Prosper\n",
    "\n",
    "## Introduction\n",
    "Let $x_t \\equiv x(t) \\in \\mathbb{R}^d$ be a $d$-dimensional vector defined on the domain $t \\in [0, 1]$. Let $x_0 \\sim p(x)$ be the **initial state** sampled from the probability density $p(x)$ and $x_1$ be the **terminal state**, that is, the state at the end of the trajectory that begins with $x_0$. Following the notation of [1], the path from $x_0$ to $x_1$ is the solution of the **forward stochastic differential equation** (SDE)\n",
    "\\begin{align}\n",
    "    dx_t = {\\color{blue}f(t) x_t dt} + {\\color{magenta}g(t) dW_t},\n",
    "\\end{align}\n",
    "where ${\\color{blue}f(t) x_t dt}$, the **drift term**, is deteministic and ${\\color{magenta}g(t) dW_t} = g(t) z \\sqrt{dt}$ is a **stochastic term** with $z \\sim {\\cal N}(0, \\mathbf{I}_d)$ a $d$-dimensional random variable from a diagonal $d$-dimensional standard normal.  Because of the stochastic term $x_t$ is also a random variable. One might be tempted to write the above equation in terms of $dx_t / dt$,  however, the path from $x_0$ to $x_t$ is infinitely corrugated and, consequently, is nowhere differentiable!  \n",
    "\n",
    "In [1] it is shown that if one chooses\n",
    "\\begin{align}\n",
    "    f(t) & = \\frac{d\\log\\alpha_t}{dt}, &\\quad \\alpha_t \\equiv \\alpha(t),\\\\\n",
    "    g^2(t) & = \\frac{d\\sigma_t^2}{dt} - 2 f(t) \\sigma_t^2, &\\quad \\sigma_t \\equiv \\sigma(t), \n",
    " \\end{align}\n",
    "then the conditional probability density of $x_t$ is given by \n",
    "\\begin{align}\n",
    "p(x_t | x_0) & = {\\cal N}(x_t; \\alpha_t x_0, \\, \\sigma^2_t \\mathbf{I}), \\\\\n",
    " & = \\prod_{i=1}^d {\\cal N}(x_{t, i}; \\alpha_t x_{0,i}, \\sigma_t^2),\\\\\n",
    " & = \\frac{1}{(\\sigma \\sqrt{2\\pi})^d} \\exp\\left[ -\\frac{1}{2}\\sum_{i=1}^d \\left( \\frac{x_{t, i} - \\alpha(t) \\, x_{0, i}}{\\sigma(t)} \\right)^2 \\right],\n",
    "\\end{align}\n",
    "where $x_{t, i} \\equiv x_i(t)$ is the $i^\\text{th}$ component of the $d$-dimensional vector $x(t)$. Defining the $d$-dimensional vector\n",
    "\\begin{align}\n",
    "    z(t) & = \\frac{x(t) - \\alpha(t) \\, x(0)}{\\sigma(t)} ,\n",
    "\\end{align}\n",
    "we can write\n",
    "\\begin{align}\n",
    "p(x_t | x_0) & \\equiv p(x(t) | x(0)), \\\\\n",
    " & \\propto \\exp \\left(-\\frac{1}{2}  z^2 \\right). \n",
    "\\end{align}\n",
    "\n",
    "If $\\alpha_t$ is chosen so that it goes to zero as $t \\rightarrow 1$ and $\\sigma_t$ remains finite in that limit then the terminal state $x_1$ will be distributed according to a zero mean diagonal $d$-dimensional normal with variance $\\sigma_1^2$ irrespective of the initial state $x_0$. The $d$-dimensional Gaussian is a **fixed point** of the SDE. \n",
    "Bao *et al.* [2] suggest the following choices\n",
    "\\begin{align}\n",
    "    \\alpha_t & = 1 - t, \\\\\n",
    "    \\sigma_t^2 & = t .\n",
    "\\end{align}\n",
    "We use\n",
    "\\begin{align}\n",
    "    \\alpha_t & = 1 - t, \\\\\n",
    "    \\sigma_t & = t .\n",
    "\\end{align}\n",
    "\n",
    "## Reverse-Time Ordinary Differential Equation (ODE)\n",
    "Even more remarkable than the above is the existence of a mechanism to go from $x_1$ to $x_0$.\n",
    "In [2] Feng Bao *et al.* note the striking mathematical fact that the reverse-time ODE\n",
    "\\begin{align}\n",
    "    dx_t & = \\left[f(t) x_t - \\frac{1}{2} g^2(t) S(t, x_t)\\right] dt, \\quad S(t, x_t) \\equiv \\nabla_{x_t} \\log p(x_t),\n",
    "\\end{align}\n",
    "where $p(x_t)$ is the probability density of $x_t$ and $S(x_t, t)$ is the **score funtion** associated with $p(x_t)$ can be used \n",
    "to map $x_t$ back to $x_0$ *deterministically*.  This is remarkable! Moreover, because these $d$ equations are ordinary differential equations we can rewrite the above as\n",
    "\\begin{align}\n",
    "    \\frac{d x_t}{dt} &= G(t, x_t), \\, \\, \\text{ where} \\\\\n",
    "    G(t, x_t) & = f(t) x_t - \\frac{1}{2} g^2(t) S(t, x_t) .\n",
    "\\end{align}\n",
    "The key technical task in using this equation is computing the score function, \n",
    "\\begin{align}\n",
    "    S(t, x_t)\n",
    "                & = \\frac{1}{p(x_t)}\\nabla_{x_t} p(x_t), \\quad p(x_t) = \\int_{\\mathbb{R}^d}   p(x_t | x_0) \\, p(x_0) \\, dx_0, \\nonumber\\\\\n",
    "        & = -\\frac{1}{p(x_t)}\\int_{\\mathbb{R}^d} \\left(\\frac{x_t - \\alpha_t x_0}{\\sigma_t^2} \\right) p(x_t | x_0) \\, p(x_0) \\, dx_0,\\nonumber\\\\\n",
    "        & = - \\frac{1}{\\sigma_t^2} \\left(x_t - \\alpha_t \\int_{\\mathbb{R}^d}  x_0 \\, \\frac{p(x_t | x_0) \\, p(x_0)}{p(x_t)} \\, dx_0 \\right),\\\\\n",
    "\\rightarrow \\, -\\sigma_t^2 \\,     S(t, x_t)        & = x_t - \\alpha_t \\, q(t, x_t),\n",
    "\\end{align}\n",
    "where the vector\n",
    "\\begin{align}\n",
    " q(t, x_t) = \\int_{\\mathbb{R}^d}  x_0 \\, p(x_0 | x_t) \\, dx_0,  \n",
    "\\end{align}\n",
    "is the conditional expectation of the state $x_0$ at $t = 0$ given the state $x_t$ at time $t$. Following [1], $g^2(t)$ can be re-expressed as follows,\n",
    "\\begin{align}\n",
    "    g^2(t) & = \\frac{d\\sigma_t^2}{dt} - 2 f(t) \\sigma_t^2,\\nonumber\\\\\n",
    "    & = 2 \\sigma_t \\frac{d\\sigma_t}{dt} - 2 \\frac{d\\log\\alpha_t}{dt} \\sigma_t^2,\\nonumber\\\\\n",
    "    & = 2 \\sigma_t^2 \\frac{d\\log\\sigma_t}{dt} - 2 \\frac{d\\log\\alpha_t}{dt} \\sigma_t^2,\\nonumber\\\\\n",
    "\\rightarrow \\quad    -\\frac{1}{2} g^2(t) & = \\sigma_t^2 \\frac{d\\log\\alpha_t/\\sigma_t}{dt}.\n",
    "\\end{align}\n",
    "Then we can rewrite $G(t, x_t)  = f(t) x_t - \\frac{1}{2} g^2(t) S(t, x_t)$ as follows\n",
    "\\begin{align}\n",
    " G(t, x_t) & = f(t) x_t - \\frac{1}{2} g^2(t) S(t, x_t),\\nonumber\\\\\n",
    "     & = \\frac{d\\log\\alpha_t}{dt} \\, x_t +  \\frac{d\\log\\alpha_t/\\sigma_t}{dt} \n",
    "     \\sigma_t^2 S(t, x_t),\n",
    "     \\nonumber\\\\\n",
    "     & = \\frac{d\\log\\alpha_t}{dt} \\, x_t + \\frac{d\\log\\alpha_t/\\sigma_t}{dt} \n",
    "     (\\alpha_t \\, q(t, x_t) - x_t), \n",
    "     \\nonumber\\\\\n",
    "     & = \\frac{d\\log\\sigma_t}{dt} \\, x_t  + \\alpha_t \\, \\frac{d\\log\\alpha_t/\\sigma_t}{dt} \n",
    "     \\, q(t, x_t), \n",
    "     \\nonumber\\\\\n",
    "     & = \\lambda(t) \\, x_t + \\mu(t)\n",
    "     \\, q(t, x_t), \\text{ where}\n",
    "     \\\\ \\nonumber\\\\\n",
    "   \\lambda(t) = \\lambda_t  & \\equiv \\frac{d\\log\\sigma_t}{dt} \\text{ and }\\\\\n",
    "   \\mu(t) = \\mu_t & \\equiv \\alpha_t \\frac{d\\log\\alpha_t/\\sigma_t}{dt}.\n",
    "\\end{align}\n",
    "\n",
    "If we set $\\alpha_t = 1 - t$ and $\\sigma_t = t$, we have $\\lambda(t) = t^{-1}$ and $\\mu(t) = -t^{-1}$ and, therefore,\n",
    "\\begin{align}\n",
    "    \\frac{dx_t}{dt} &= \\lambda_t \\, x_t + \\mu_t \\, q(t, x_t),\\nonumber\\\\\n",
    "    \\frac{dx_t}{dt} &= [ x_t  - q(t, x_t) ] t^{-1} \\equiv G(t, x_t).\n",
    "\\end{align}\n",
    "\n",
    "## Approximating $q(t, x_t)$\n",
    "Bao *et al.* propose to approximate the integrals over $\\mathbb{R}^d$ using Monte Carlo integration. Writing \n",
    "$q(x_t)$ as\n",
    "\\begin{align}\n",
    "q(t, x_t) & = \\int_{\\mathbb{R}^d}  x_0 \\, p(x_0 | x_t) \\, dx_0 , \\\\\n",
    "& =  \\int_{\\mathbb{R}^d}  x_0 \\, \\frac{p(x_t | x_0) \\, p(x_0)}{p(x_t)} \\, dx_0,\\\\\n",
    "                    & = \\int_{\\mathbb{R}^d}  x_0 \\, \\lambda(x_t, x_0) \\, p(x_0) \\, dx_0 ,  \\quad\\text{where the ratio } \\lambda(x_t, x_0) = \\frac{p(x_t | x_0)}{p(x_t)},\n",
    "\\end{align}\n",
    "we can approximate the integrals as follows, \n",
    "\\begin{align}\n",
    "        q(t, x_t)  & \\approx \\frac{1}{N} \\sum_{n=1}^N x_0^{(n)} \\, \\lambda(x_t, x_0^{(n)}), \\quad \\lambda(x_t, x_{0,n}) \\approx \\frac{p(x_t | x_0^{(n)})}{\\frac{1}{N}\\sum_{m=1}^N p(x_t | x_0^{(m)})},\\\\\n",
    "        & =  \\sum_{n=1}^N x_0^{(n)} \\, w(x_t, x_0^{(n)}), \n",
    "        \\quad\\text{where } \\\\\n",
    "w(x_t, x_0^{(n)}) & = \\frac{p(x_t | x_0^{(n)})}{\\sum_{m=1}^N p(x_t | x_0^{(m)})},                \n",
    "\\end{align}\n",
    "and $\\{ x_0^{(n)} \\}, n = 1,\\cdots, N$ is a sample drawn from $p(x_0)$. \n",
    "\n",
    "The function $\\lambda(x_t, x_0)$ has an interesting interpretation as can be seen by writing it as follows,\n",
    "\\begin{align}\n",
    "    \\lambda(x_t, x_0) & = \\frac{p(x_t| x_0)}{p(x_t)}, \\nonumber\\\\\n",
    "    & = \\frac{p(x_t| x_0) \\, p(x_0)}{p(x_t) \\, p(x_0)}, \\nonumber\\\\\n",
    "    & = \\frac{p(x_t, x_0)}{p(x_t) \\, p(x_0)}.\n",
    "\\end{align}\n",
    "The function $\\lambda(x_t, x_0)$ measures the degree to which the variables $x_t$ and $x_0$ are statistically independent.\n",
    "\n",
    "### An alternative approach\n",
    "Notice that\n",
    "\\begin{align}\n",
    "q(t, x_t) \n",
    "& =  \\int_{\\mathbb{R}^d}  x_0 \\, \\frac{p(x_t | x_0) \\, p(x_0)}{p(x_t)} \\, dx_0,\\nonumber\\\\\n",
    "& =  \\frac{\\int_{\\mathbb{R}^d}  x_0 \\, p(x_t | x_0) \\, p(x_0) \\, dx_0}{\\int_{\\mathbb{R}^d}  \\, p(x_t | x_0) \\, p(x_0) \\, dx_0}.\n",
    "\\end{align}\n",
    "This suggests that it may be useful to define the vector-valued function, $I$, where the components $I_i$, $i = 1,\\cdots, d$ are given by\n",
    "\\begin{align}\n",
    "I_i(t, x_t, \\beta) \n",
    "& \\equiv  \\int_{\\mathbb{R}^d}  e^{-\\beta x_{0, i}} \\, p(x_t | x_0) \\, p(x_0) \\, dx_0, \\nonumber\\\\\n",
    "    & = \\mathbb{E}_{p(x_0)}[e^{-\\beta x_{0, i}} \\, p(x_t | x_0) ], \\\\\n",
    "   & \\approx \\frac{1}{N} \\sum_{n=1}^N \\exp\\left[-\\left(\\beta x_{0,i}^{(n)} + \\frac{1}{2} z^{(n)} \\cdot z^{(n)} \\right)\\right], \\quad z^{(n)} = \\frac{x_t - \\alpha(t) \\, x_0^{(n)}}{\\sigma(t)}.\n",
    "\\end{align}\n",
    "We can then write\n",
    "\\begin{align}\n",
    "q(t, x_t) & = -\\left. \\frac{1}{I(t, x_t, \\beta)} \\frac{d I(t, x_t, \\beta)}{d\\beta}\\right|_{\\beta=0}  , \\nonumber\\\\\n",
    "     & = -\\left. \\frac{d \\log I(t, x_t, \\beta)}{d\\beta}\\right|_{\\beta=0} .\n",
    "\\end{align}\n",
    "\n",
    "## Implicit solution\n",
    "\n",
    "If multiply\n",
    "\\begin{align}\n",
    "    \\frac{dx_t}{dt} &= [ x_t  - q(t, x_t) ] t^{-1} \\equiv G(t, x_t), \\nonumber\n",
    "\\end{align}\n",
    "throughout by the integrating factor $t^{-1}$, we can write\n",
    "\\begin{align}\n",
    "    t^{-1}\\frac{dx_t}{dt} -  t^{-2} x_t & = - t^{-2} \\, q(t, x_t) , \\nonumber\\\\\n",
    "    \\frac{d}{dt} t^{-1} x_t & = - t^{-2} \\, q(t, x_t) , \\\\\n",
    "\\end{align}\n",
    "and, therefore, arrive at\n",
    "\\begin{align}\n",
    "    t_2^{-1} x_{t_2}  & = t_1^{-1} x_{t_1}  - \\int_{s=t_1}^{s=t_2} \\frac{q(s, x_s)}{s^2} \\, ds, \\\\\n",
    "    x_{t_2}  & = \\frac{t_2}{t_1} x_{t_1} - t_2 \\int_{s=t_1}^{s=t_2} \\, \\frac{q(s, x_s)}{s^2} \\, ds.\n",
    "\\end{align}\n",
    "\n",
    "### References\n",
    "  1. Cheng Lu†, Yuhao Zhou†, Fan Bao†, Jianfei Chen†, Chongxuan Li‡, Jun Zhu, *DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps*, arXiv:2206.00927v3, 13 Oct 2022.\n",
    "  1. Yanfang Lui, Minglei Yang, Zezhong Zhang, Feng Bao, Yanzhao Cao, and Guannan Zhang, *Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation*, arXiv:2310.14458v1, 22 Oct 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a031fdf-7e4b-40a6-ba3a-93d630ccc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# standard research-level machine learning toolkit from Meta (FKA: FaceBook)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# standard module for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b7aead-eb96-4c9c-b9fe-b228ac17e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tComputational device: cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\n\\tComputational device: {str(DEVICE):4s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9337d-8e39-46e1-b664-90111c70d6c8",
   "metadata": {},
   "source": [
    "### Reverse Time ODE\n",
    "\n",
    "Specializing to $\\alpha_t = 1 - t$ and $\\sigma_t = t$, we have $\\lambda(t) = t^{-1}$ and $\\mu(t) = -t^{-1}$ and, therefore,\n",
    "\\begin{align}\n",
    "    \\frac{dx_t}{dt} &= \\lambda_t \\, x_t + \\mu_t \\, q(t, x_t),\\nonumber\\\\\n",
    "    \\frac{dx_t}{dt} &= [ x_t  - q(t, x_t) ] t^{-1} \\equiv G(t, x_t),\n",
    "\\end{align}\n",
    "\n",
    "#### Numerical approximation\n",
    "Given \n",
    "\\begin{align}\n",
    "    G(t - h) & = G(t) - G^\\prime h + {\\cal O}(h^2),\\\\\n",
    "    \\rightarrow G^\\prime h & = G(t) - G(t - h) + {\\cal O}(h^2).\n",
    "\\end{align}\n",
    "Then,\n",
    "\\begin{align}\n",
    "    x(t - h) \n",
    "    & = x(t) - x^\\prime h + \\frac{1}{2!} x^{\\prime\\prime} h^2 - {\\cal O}(h^3),\\\\\n",
    "    & = x(t) - G h + \\frac{1}{2}\\left[G(t, x(t)) - G(t - h, x(t-h)) \\right]h - {\\cal O}(h^3),\\nonumber\\\\\n",
    "    & = x(t) - \\frac{1}{2}[G(t, x_t) + G(t - h, x_{t-h}) ] h - {\\cal O}(h^3) ,\\\\\n",
    "    & = x_t - \\frac{1}{2}[G(t, x_t) + G(t - h, x_t - G(t, x_t) h + {\\cal O}(h^2)) ] h - {\\cal O}(h^3) .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee25b544-b2f2-4240-a3c8-565b7d7ba41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_sample(x):\n",
    "    means = np.zeros_like(x)\n",
    "    scales = np.ones_like(x)\n",
    "    return torch.Tensor(np.random.normal(loc=means, scale=scales))\n",
    "        \n",
    "def get_batch(x, size=4000):\n",
    "    ii = np.random.randint(0, len(x)-1, size)\n",
    "    return torch.Tensor(x[ii])\n",
    "\n",
    "class PyFlowODE(nn.Module):    \n",
    "    '''\n",
    "    Given Gaussian states x1 at t=1 compute the desired state x0 at t=0 \n",
    "    by mapping x1 to x0 deterministically. x0, which is of shape (B, d), \n",
    "    where B is the batch size and d the dimension of the vector x0 = x(0), \n",
    "    is used to compute a Monte Carlo approximation of the q function. The \n",
    "    tensor x1 is of shape (N, d), where N is the number of points sampled \n",
    "    from the Gaussian. Given the numpy array X0, x0 and x1 can be created\n",
    "    as follows,\n",
    "        \n",
    "        x0 = get_batch(X0, B).to(device)\n",
    "        x1 = get_normal_sample(X0).to(device)\n",
    "\n",
    "    Utility functions\n",
    "    =================\n",
    "    1. get_normal_sample(X0) returns a tensor x1, with the same shape as X0, \n",
    "    whose elements are sampled from a diagonal d-dimensional Gaussian. \n",
    "\n",
    "    2. get_batch(X0, B) returns a batch of points, x0, of size B from X0,\n",
    "    which will be used to approximate the q-function.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "\n",
    "    B = 5000\n",
    "    N = 1000\n",
    "    \n",
    "    x0 = get_batch(X0, B)).to(device)\n",
    "    x1 = get_normal_sample(X0[:N]).to(device)\n",
    "    \n",
    "    ode = PyFlowODE(x0)\n",
    "\n",
    "    Y = ode(x1)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, x0, T=1000, savepath=False, debug=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if len(x0.shape) != 2:\n",
    "            raise IndexError('''\n",
    "    The batch of data should be of shape (B, d), where B is\n",
    "    the batch size and d is the dimensionality of the space.\n",
    "            ''')\n",
    "\n",
    "        if T < 4: T = 4\n",
    "\n",
    "        # B: batch size\n",
    "        # d: dimensionality of space\n",
    "        self.B, self.d = x0.shape\n",
    "        \n",
    "        # change shape of x0 from (B, d) to (1, B, d)\n",
    "        # so that broadcasting works correctly later.\n",
    "        self.x0 = x0.unsqueeze(0)\n",
    "        \n",
    "        self.T = T\n",
    "        self.h = 1/T\n",
    "        self.savepath = savepath\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            print('PyFlowODE.__init__: x0.shape', x0.shape)\n",
    "            print('PyFlowODE.__init__: h', self.h)\n",
    "\n",
    "    def set_debug(self, debug=True):\n",
    "        self.debug = debug\n",
    "        \n",
    "    def q(self, t, xt):\n",
    "\n",
    "        debug = self.debug\n",
    "            \n",
    "        x0 = self.x0\n",
    "        sigma_t = t\n",
    "        alpha_t = 1-t\n",
    "\n",
    "        if debug:\n",
    "            print('PyFlowODE.q(BEGIN)')\n",
    "            print('  PyFlowODE.q: xt.shape', xt.shape)\n",
    "            print('  PyFlowODE.q: x0.shape', x0.shape)\n",
    "            \n",
    "        z = (xt - alpha_t * x0) / sigma_t\n",
    "        # z.shape: (N, B, d)\n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: z.shape', z.shape)\n",
    "            print('  PyFlowODE.q: z', z)\n",
    "\n",
    "        # sum over arguments of exponential, that is, over the d-dimensions\n",
    "        # of each element in x0, so that we get the product of d normal densities.    \n",
    "        zz = z*z\n",
    "        zz = zz.sum(dim=-1) # sum over last axis\n",
    "        \n",
    "        # to reduce the probability of overflows,\n",
    "        # subtract from zz its manimum value. This is\n",
    "        # ok because exp(-zz.min()/2) will cancel in the\n",
    "        # weight.\n",
    "        zzmin = zz.min()\n",
    "        zzmax = zz.max()\n",
    "        zz -= zzmin\n",
    "        \n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: zz.shape', zz.shape)\n",
    "            print('  PyFlowODE.q: zzmin, zzmax', zzmin, zzmax)\n",
    "            print('  PyFlowODE.q: zz', zz)\n",
    " \n",
    "        # compute unnormalized probability densities\n",
    "        pt = torch.exp(-zz/2)\n",
    "        # pt.shape: (N, B)\n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: pt.shape', pt.shape)\n",
    "            print('  PyFlowODE.q: pt', pt)\n",
    "\n",
    "        # compute weight by dividing by the sum over the B Gaussian densities,\n",
    "        # which, in general, is the last axis\n",
    "        pt_sum = pt.sum(dim=-1)\n",
    "        \n",
    "        # pt_sum.shape: (N, )\n",
    "        if pt_sum.any() == 0:\n",
    "            raise ValueError(f'''\n",
    "   pt_sum is zero at time step {t:10.3f}\n",
    "            ''')\n",
    "\n",
    "        # pt.shape: (N, B)\n",
    "        # pt_sum.shape: (N, ) => (N, 1)\n",
    "        wt = pt / pt_sum.unsqueeze(1)\n",
    "            \n",
    "        # wt.shape: (N, B)\n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: wt.shape', wt.shape)\n",
    "        \n",
    "        # sum over the batch of B weighted elements of x0\n",
    "        # this amounts to summing over the first axis of the array x0.\n",
    "        # x0.shape: (1, B, d)\n",
    "        # wt.shape: (N, B) => (N, B, 1)\n",
    "        x0_wt = x0 * wt.unsqueeze(2)\n",
    "        # x0_wt.shape: (N, B, d)\n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: x0_wt.shape', x0_wt.shape)\n",
    "\n",
    "        # sum over the batch dimension of x0_wt and reshape qt\n",
    "        # so that it has the same shape as xt\n",
    "        # qt.shape: (N, d) => (N, 1, d)\n",
    "        qt = x0_wt.sum(dim=-2).unsqueeze(1)\n",
    "        if debug:\n",
    "            print('  PyFlowODE.q: qt.shape', qt.shape)\n",
    "            print('PyFlowODE.q(END)')\n",
    "        return qt\n",
    "\n",
    "    def score(self, t, xt):\n",
    "        qt = self.q(t, xt) # qt.shape: (N, 1, d)\n",
    "        # xt and qt must be of the same shape.\n",
    "        # finish calculation of the score\n",
    "        st = -(xt - (1-t) * qt)/ t / t\n",
    "        return st\n",
    "\n",
    "    def G(self, t, xt):\n",
    "        qt = self.q(t, xt)\n",
    "        return (xt - qt)/t\n",
    "\n",
    "    def forward(self, x1):\n",
    "\n",
    "        debug = self.debug\n",
    "        \n",
    "        # check shape of input data\n",
    "        if len(x1.shape) < 2:\n",
    "            xt = x1.unsqueeze(0).unsqueeze(0) # shape: (1, 1, d)\n",
    "        else:\n",
    "            # since xt is of shape (N, d), change\n",
    "            # shape of xt to (N, 1, d) and so that \n",
    "            # broadcasting with x0 (of shape (1, B, d)) \n",
    "            # works correctly.\n",
    "            xt = x1.unsqueeze(1)\n",
    "\n",
    "        N, _, d = xt.shape\n",
    "        if d != self.d:\n",
    "            raise ValueError('''\n",
    "        The dimension of x1 must match that of x0.\n",
    "            ''')\n",
    "            \n",
    "        savepath = self.savepath\n",
    "        T = self.T\n",
    "        h = self.h\n",
    "        t = 1\n",
    "\n",
    "        if debug:\n",
    "            print('PyFlowODE.__call__: xt.shape', xt.shape)\n",
    "            print('PyFlowODE.__call__: t', t)\n",
    "\n",
    "        if savepath:\n",
    "            y = [xt.squeeze(1)]\n",
    "\n",
    "        G1 = self.G(t, xt)\n",
    "        \n",
    "        for i in tqdm(range(T-1)):\n",
    "            t -= h\n",
    "            if t <= 0: \n",
    "                break\n",
    "                \n",
    "            if debug:\n",
    "                print('PyFlowODE.__call__: t', t)\n",
    "                print('PyFlowODE.__call__: xt.shape', xt.shape) \n",
    "                print('PyFlowODE.__call__: G1.shape', G1.shape)\n",
    "                \n",
    "            G2 = self.G(t, xt - G1 * h)\n",
    "\n",
    "            xt = xt - (G1 + G2) * h / 2\n",
    "            \n",
    "            G1 = G2.detach().clone()\n",
    "\n",
    "            if savepath:\n",
    "                y.append(xt.squeeze(1))\n",
    "\n",
    "        if savepath:\n",
    "            return y\n",
    "        else:\n",
    "            return xt.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ac83a1-d7e6-479e-bbfe-7ee9b2eb4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow2DAnimation:\n",
    "    '''\n",
    "    Given a Python list, y, of arrays (or tensors), each of shape (N, d) where N is the\n",
    "    number of data points and d=2 is the dimensionality of the space, animate the flow\n",
    "    of points from a diagonal 2D, zero mean, unit variance Gaussian to the target\n",
    "    distribution.\n",
    "\n",
    "    anim = Flow2DAnimation(y)\n",
    "\n",
    "    The animation can be saved as a gif or, if ffmpeg is available, in mp4 format using\n",
    "\n",
    "    anim.save('flow.gif')\n",
    "    anim.save('flow.mp4')\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        data :      Python list of data objects. Each object is table (either a 2D numpy \n",
    "                    array or a 2D tensor) or shape (N, d). Each data object corresponds\n",
    "                    to a time step.\n",
    "        xmin, xmax: x limits [-5, 5]\n",
    "        ymin, ymax: y limits [-5, 5]\n",
    "        nframes :   number of frames [50]\n",
    "        interval :  time between frames in milliseconds [100 ms]\n",
    "        mcolor :    marker color ['blue']\n",
    "        msize :     marker size [1]\n",
    "        fgsize :    size of figure [(4, 4)]\n",
    "        ftsize :    font size [16 pt]\n",
    "    '''\n",
    "    def __init__(self, data,\n",
    "                 xmin=-5, xmax=5, ymin=-5, ymax=5,\n",
    "                 nframes=50,     # number of frames\n",
    "                 interval=100,   # time between frames in milliseconds\n",
    "                 mcolor='blue',  # color of points\n",
    "                 msize=1,\n",
    "                 fgsize=(4, 4), \n",
    "                 ftsize=16):\n",
    "        \n",
    "        # cache inputs\n",
    "        self.data = np.array(data)\n",
    "        self.nframes = nframes\n",
    "        self.interval = interval\n",
    "        self.T = len(self.data)\n",
    "        self.npoints, self.d = self.data[0].shape\n",
    "        self.factor = self.T / nframes\n",
    "        \n",
    "        # set size of figure\n",
    "        self.fig = plt.figure(figsize=fgsize)\n",
    "\n",
    "        # create area for a plot \n",
    "        nrows, ncols, index = 1, 1, 1\n",
    "        ax  = plt.subplot(nrows, ncols, index)\n",
    "        \n",
    "        self.ax = ax   # cache plot\n",
    "        \n",
    "        ax.set_title('Reverse Time Diffusion', pad=14)\n",
    "\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "        \n",
    "        # create a scatter plot \n",
    "        x = self.data[0, :, 0] # initial x positions\n",
    "        y = self.data[0, :, 1] # initial y positions\n",
    "       \n",
    "        self.scatter = ax.scatter(x, y, s=msize, c=mcolor, alpha=1)\n",
    "\n",
    "        # matplotlib refers to the objects scatter, etc., as artists.\n",
    "        # we need to place artists in a list since we need to return all of\n",
    "        # them in the function update() for the animation to work correctly\n",
    "        self.artists = []\n",
    "        self.artists.append(self.scatter)\n",
    "\n",
    "        # IMPORTANT: Turn off Latex processing of text; it is far too slow!\n",
    "        mp.rc('text', usetex=False)\n",
    "\n",
    "        # create a text object to display the days since the start\n",
    "        self.text = ax.text(0.95*xmin, 0.80*ymin, f't: {0:5.2f}')\n",
    "        self.artists.append(self.text)\n",
    "\n",
    "        # fix the layout so that labels are a bit\n",
    "        # closer to the plot\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        # don't show the above plots. Show only the animated version\n",
    "        plt.close()\n",
    "\n",
    "        # initialize animated plot\n",
    "        self.ani = FuncAnimation(fig=self.fig,           # animation figure\n",
    "                                 func=self.update,       # function to update plot in figure\n",
    "                                 repeat=False,           # don't repeat animation\n",
    "                                 frames=self.nframes,    # number of frames\n",
    "                                 interval=self.interval) # time between frames (ms)\n",
    "\n",
    "    # this is the function that updates the plot    \n",
    "    def update(self, frame):\n",
    "        \n",
    "        print(f'\\rframe: {frame:d}', end='')\n",
    "        \n",
    "        # get data and artists!\n",
    "        data, scatter, text = self.data, self.scatter, self.text\n",
    "        \n",
    "        # compute index into data array (i.e., time step)\n",
    "        index = np.ceil(frame * self.factor).astype(int)\n",
    "        t = 1 - frame / (self.nframes-1)\n",
    "        \n",
    "        # display the number of days\n",
    "        self.text.set_text(f't: {t:5.2f}')\n",
    "        \n",
    "        # display the current position of points\n",
    "        x = data[index, :, 0]\n",
    "        y = data[index, :, 1]\n",
    "        a = np.array([x, y]).T   # we need x and y to be columns\n",
    "        scatter.set_offsets(a)\n",
    "\n",
    "        # must return artists for animation to work\n",
    "        return self.artists\n",
    "    \n",
    "    def show(self):\n",
    "        '''\n",
    "        Construct and show the animation.\n",
    "        '''\n",
    "        plt.show()\n",
    "        return self.ani\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''\n",
    "        Construct and save the animation to a file. The format of the file is\n",
    "        determined from the file extension. For example, a filename with \n",
    "        extension \".gif\" saves the animation as an animated gif, while one with\n",
    "        extension \".mp4\" saves it in mp4 format. Note: the latter requires the\n",
    "        module ffmpeg.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "        filename :      Name of graphics file.\n",
    "        '''\n",
    "        self.ani.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1941997-1c99-4edf-a8ef-4c34d9d9585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
